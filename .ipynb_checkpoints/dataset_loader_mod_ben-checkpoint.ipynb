{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_helper_triangle\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_helper_triangle\n",
    "# %load data_helper.py\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from helper import convert_map_to_lane_map, convert_map_to_road_map\n",
    "\n",
    "NUM_SAMPLE_PER_SCENE = 126\n",
    "NUM_IMAGE_PER_SAMPLE = 6\n",
    "image_names = [\n",
    "    'CAM_FRONT_LEFT.jpeg',\n",
    "    'CAM_FRONT.jpeg',\n",
    "    'CAM_FRONT_RIGHT.jpeg',\n",
    "    'CAM_BACK_LEFT.jpeg',\n",
    "    'CAM_BACK.jpeg',\n",
    "    'CAM_BACK_RIGHT.jpeg',\n",
    "    ]\n",
    "\n",
    "#BASICALLY YOU JUST HAVE TO SPECIFY THE IMAGE TO LPAD IN THE __GET_ITEM__ on init\n",
    "# The dataset class for labeled data.\n",
    "class TriangleLabeledDataset(torch.utils.data.Dataset):    \n",
    "    def __init__(self, image_folder, annotation_file, scene_index, transform,extra_info=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_folder (string): the location of the image folder\n",
    "            annotation_file (string): the location of the annotations\n",
    "            scene_index (list): a list of scene indices for the unlabeled data \n",
    "            transform (Transform): The function to process the image\n",
    "            extra_info (Boolean): whether you want the extra information\n",
    "        \"\"\"\n",
    "        assert(camera in image_names)\n",
    "        self.image_folder = image_folder\n",
    "        self.annotation_dataframe = pd.read_csv(annotation_file)\n",
    "        self.scene_index = scene_index\n",
    "        self.transform = transform\n",
    "        self.extra_info = extra_info\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.scene_index.size * NUM_SAMPLE_PER_SCENE\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index,tuple):\n",
    "            index,camera = index\n",
    "            assert(camera in image_names)\n",
    "            camera_index = True\n",
    "        scene_id = self.scene_index[index // NUM_SAMPLE_PER_SCENE]\n",
    "        sample_id = index % NUM_SAMPLE_PER_SCENE\n",
    "        sample_path = os.path.join(self.image_folder, f'scene_{scene_id}', f'sample_{sample_id}') \n",
    "        \n",
    "        #get camera and only get that image\n",
    "        image_path = os.path.join(sample_path, camera)\n",
    "        image = Image.open(image_path)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        data_entries = self.annotation_dataframe[(self.annotation_dataframe['scene'] == scene_id) & (self.annotation_dataframe['sample'] == sample_id)]\n",
    "        corners = data_entries[['fl_x', 'fr_x', 'bl_x', 'br_x', 'fl_y', 'fr_y','bl_y', 'br_y']].to_numpy()\n",
    "        categories = data_entries.category_id.to_numpy()\n",
    "        \n",
    "        ego_path = os.path.join(sample_path, 'ego.png')\n",
    "        ego_image = Image.open(ego_path)\n",
    "        ego_image = torchvision.transforms.functional.to_tensor(ego_image)\n",
    "        road_image = convert_map_to_road_map(ego_image)\n",
    "        \n",
    "        target = {}\n",
    "        target['bounding_box'] = torch.as_tensor(corners).view(-1, 2, 4)\n",
    "        target['category'] = torch.as_tensor(categories)\n",
    "\n",
    "        if self.extra_info:\n",
    "            actions = data_entries.action_id.to_numpy()\n",
    "            # You can change the binary_lane to False to get a lane with \n",
    "            lane_image = convert_map_to_lane_map(ego_image, binary_lane=True)\n",
    "            \n",
    "            extra = {}\n",
    "            extra['action'] = torch.as_tensor(actions)\n",
    "            extra['ego_image'] = ego_image\n",
    "            extra['lane_image'] = lane_image\n",
    "\n",
    "            return image, target, road_image, extra\n",
    "        \n",
    "        else:\n",
    "            return image, target, road_image\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
