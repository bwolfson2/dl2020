{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#for image transform\n",
    "import cv2\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "from model_loader_CP2 import *\n",
    "#from CP_helper import *\n",
    "from Unet import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.67 ms\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 637 ms\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 594 µs\n"
     ]
    }
   ],
   "source": [
    "image_folder = 'data'\n",
    "annotation_csv = 'data/annotation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 786 µs\n"
     ]
    }
   ],
   "source": [
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "labeled_scene_index = np.arange(106, 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118,\n",
       "       119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131,\n",
       "       132, 133])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.78 ms\n"
     ]
    }
   ],
   "source": [
    "labeled_scene_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.65 ms\n"
     ]
    }
   ],
   "source": [
    "len(labeled_scene_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 637 µs\n"
     ]
    }
   ],
   "source": [
    "labeled_scene_index_shuf = labeled_scene_index\n",
    "random.shuffle(labeled_scene_index_shuf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([109, 120, 116, 123, 111, 106, 113, 127, 125, 124, 129, 131, 110,\n",
       "       108, 128, 112, 117, 126, 115, 132, 121, 122, 114, 107, 119, 130,\n",
       "       118, 133])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.79 ms\n"
     ]
    }
   ],
   "source": [
    "labeled_scene_index_shuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 669 µs\n"
     ]
    }
   ],
   "source": [
    "train_labeled_scene_index = labeled_scene_index_shuf[:-10]\n",
    "val_labeled_scene_index = labeled_scene_index_shuf[-10: 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([109, 120, 116, 123, 111, 106, 113, 127, 125, 124, 129, 131, 110,\n",
       "       108, 128, 112, 117, 126])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.72 ms\n"
     ]
    }
   ],
   "source": [
    "train_labeled_scene_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.4 ms\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    #'first_dim': 'sample',\n",
    "    'transform': transforms.ToTensor(),\n",
    "    'image_folder': image_folder,\n",
    "    'annotation_file': annotation_csv,\n",
    "    'extra_info': True}\n",
    "\n",
    "def gen_train_val_loader(labeled_scene_index, **kwargs):\n",
    "    labeled_scene_index_shuf = labeled_scene_index\n",
    "    random.shuffle(labeled_scene_index_shuf)\n",
    "    train_labeled_scene_index = labeled_scene_index_shuf[:-10] #hard code we know there are only 28 scenes that are labeled\n",
    "    val_labeled_scene_index = labeled_scene_index_shuf[-10:]\n",
    "    \n",
    "    print(len(train_labeled_scene_index), len(val_labeled_scene_index))\n",
    "    print(train_labeled_scene_index[0], val_labeled_scene_index[0])\n",
    "        \n",
    "    loadkwargs = {'batch_size': 2,\n",
    "    'shuffle': True,\n",
    "    'collate_fn':collate_fn,\n",
    "    'num_workers':2,\n",
    "    \n",
    "    }\n",
    "    \n",
    "    labeled_trainset = LabeledDataset(scene_index=train_labeled_scene_index, **kwargs)\n",
    "    print(len(labeled_trainset))\n",
    "    trainloader = torch.utils.data.DataLoader(labeled_trainset, **loadkwargs)\n",
    "    \n",
    "    labeled_valset = LabeledDataset(scene_index=val_labeled_scene_index, **kwargs)  \n",
    "    print(len(labeled_valset))\n",
    "    valloader = torch.utils.data.DataLoader(labeled_valset, **loadkwargs)\n",
    "    \n",
    "    result={\"train\" : trainloader,\n",
    "           \"val\": valloader\n",
    "        \n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 10\n",
      "110 117\n",
      "2268\n",
      "1260\n",
      "time: 253 ms\n"
     ]
    }
   ],
   "source": [
    "train_val_loader = gen_train_val_loader(labeled_scene_index, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([6, 3, 256, 306])\n",
      "torch.Size([800, 800])\n",
      "time: 306 ms\n"
     ]
    }
   ],
   "source": [
    "train_loader = train_val_loader[\"train\"]\n",
    "for i ,(sample, target, road_image, extra) in enumerate(train_loader):\n",
    "    print(len(sample))\n",
    "    print(sample[0].shape)\n",
    "    print(road_image[0].shape)\n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.44 ms\n"
     ]
    }
   ],
   "source": [
    "train_kwargs={\n",
    "    'epochs':1,\n",
    "    \"lr\": 0.01,\n",
    "    'momentum': 0.99\n",
    "    }\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "     \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i ,(sample, target, road_image, extra) in enumerate(loader):\n",
    "             \n",
    "            sample_ = ModelLoader.get_binary_road_map(sample).cuda() #should be [batch size, 800, 800]\n",
    "            labels = torch.stack(road_image, 0).cuda() #should be [batch size, 800, 800]\n",
    "            \n",
    "            \n",
    "            outputs = model(sample_.unsqueeze(1))\n",
    "            outputs = outputs.squeeze(1)\n",
    "            predicted = (outputs>0.5).int() ## convert to bineary\n",
    "            \n",
    "            total += (labels.size(0)*labels.size(1)*labels.size(2))\n",
    "            correct += predicted.eq(labels.int()).sum().item()\n",
    "        \n",
    "    return (100 * correct / total)\n",
    "     \n",
    "\n",
    "\n",
    "def train(train_val_loader, **train_kwargs):\n",
    "    #initialize stuff...\n",
    "    train_loader = train_val_loader[\"train\"]\n",
    "    val_loader = train_val_loader[\"val\"]\n",
    "    \n",
    "    unet = UNet(in_channel=1,out_channel=1).cuda()\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    param_list = [p for p in unet.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(param_list, lr = train_kwargs[\"lr\"], momentum=train_kwargs[\"momentum\"])\n",
    "    train_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    unet.train()\n",
    "    for e in range(train_kwargs[\"epochs\"]):\n",
    "        t = time.process_time()\n",
    "\n",
    "        for i ,(sample, target, road_image, extra) in enumerate(train_loader):\n",
    "            sample_ = ModelLoader.get_binary_road_map(sample).cuda() #should be [batch size, 800, 800]\n",
    "            labels = torch.stack(road_image, 0).cuda() #should be [batch size, 800, 800]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = unet(sample_.unsqueeze(1)) #unet needs the channels dimension\n",
    "            outputs = outputs.squeeze(1)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # validate every 200 iterations\n",
    "            if i > 0 and i % 100== 0:\n",
    "                val_acc = test_model(val_loader, unet) #calls model.eval()\n",
    "                val_accs.append(val_acc)\n",
    "                #do some stuff\n",
    "                elapsed_time = time.process_time() - t\n",
    "                print('Epoch: [{}], Step: [{}], Train Loss {:.4f}, Validation Acc: {:.4f}, time {:.4f}'.format( \n",
    "                           e+1, i+1, loss,  val_acc, elapsed_time))\n",
    "                unet.train() #go back to training\n",
    "                t = time.process_time()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1], Step: [101], Train Loss 0.681217610836029, Validation Acc: 52.72410218253968\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5d812b5b5839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     }\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_val_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrain_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-b183b7b25e74>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_val_loader, **train_kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 print('Epoch: [{}], Step: [{}], Train Loss {}, Validation Acc: {}'.format( \n\u001b[1;32m     66\u001b[0m                            e+1, i+1, loss,  val_acc))\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#go back to training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min 1s\n"
     ]
    }
   ],
   "source": [
    "train_kwargs={\n",
    "    'epochs':1,\n",
    "    \"lr\": 0.01,\n",
    "    'momentum': 0.99\n",
    "    }\n",
    "\n",
    "train(train_val_loader, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfafdsafdsafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sample\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "unlabeled_trainset = UnlabeledDataset(image_folder=image_folder,scene_index=unlabeled_scene_index, first_dim='sample', transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(unlabeled_trainset, batch_size=3, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [batch_size, 6(images per sample), 3, H, W]\n",
    "sample = iter(trainloader).next()\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 6 images orgenized in the following order:\n",
    "# 0 CAM_FRONT_LEFT, 1 CAM_FRONT, 2 CAM_FRONT_RIGHT, 3 CAM_BACK_LEFT, 4 CAM_BACK, 5 CAM_BACK_RIGHT\n",
    "plt.imshow(torchvision.utils.make_grid(sample[2], nrow=3).numpy().transpose(1, 2, 0)) #need the transpose\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "# You can choose whether the loader returns the extra_info. It is optional. You don't have to use it.\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "LB_trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target, road_image, extra = iter(LB_trainloader).next()\n",
    "print(torch.stack(sample).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toImg = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "toImg(sample[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torchvision.utils.make_grid(sample[0], nrow=3).numpy().transpose(1, 2, 0)) #need the transpose\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROAD IMAGE ##\n",
    "# The road map layout is encoded into a binary array of size [800, 800] per sample \n",
    "# Each pixel is 0.1 meter in physiscal space, so 800 * 800 is 80m * 80m centered at the ego car\n",
    "# The ego car is located in the center of the map (400, 400) and it is always facing the left\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "\n",
    "for i, bb in enumerate(target[0]['bounding_box']):\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][i]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_image[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ = ModelLoader.get_binary_road_map(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min max is between 0 and 1\n",
    "print(sample_[0].min())\n",
    "print(sample_[1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "resnet18_pre = models.resnet18(pretrained=True)\n",
    "resnet18_raw = models.resnet18(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a normalize tranform \n",
    "# res_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet requires all sizes to be 224 by 224\n",
    "# res_transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_img = ModelLoader.sew_images(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_ftrs = resnet18_pre.fc.in_features\n",
    "# resnet18_pre.fc = Identity() #first set it to identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##resnet is only for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_.unsqueeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#afjkd;a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try one sample of for Unet\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "\n",
    "sing_samp_unet = sample_.unsqueeze(1)\n",
    "testUnet = UNet(in_channels, out_channels)\n",
    "testUnetout = testUnet(sing_samp_unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testUnetout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testUnetout.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(road_image[:2],0).unsqueeze(1).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((testUnetout>0.5).int()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(road_image[:2],0).unsqueeze(1).eq((testUnetout>0.5).int()).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "649756/(2*800*800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure out loss Use BCE loss\n",
    "\n",
    "\n",
    "sample, target, road_image, extra = iter(LB_trainloader).next()\n",
    "\n",
    "## ROAD IMAGE ##\n",
    "# The road map layout is encoded into a binary array of size [800, 800] per sample \n",
    "# Each pixel is 0.1 meter in physiscal space, so 800 * 800 is 80m * 80m centered at the ego car\n",
    "# The ego car is located in the center of the map (400, 400) and it is always facing the left\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "\n",
    "for i, bb in enumerate(target[0]['bounding_box']):\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][i]])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure out loss Use BCE loss\n",
    "\n",
    "\n",
    "sample, target, road_image, extra = iter(LB_trainloader).next()\n",
    "\n",
    "## ROAD IMAGE ##\n",
    "# The road map layout is encoded into a binary array of size [800, 800] per sample \n",
    "# Each pixel is 0.1 meter in physiscal space, so 800 * 800 is 80m * 80m centered at the ego car\n",
    "# The ego car is located in the center of the map (400, 400) and it is always facing the left\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "\n",
    "for i, bb in enumerate(target[0]['bounding_box']):\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][i]])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sample, target, road_image, extra = iter(LB_trainloader).next()\n",
    "\n",
    "## ROAD IMAGE ##\n",
    "# The road map layout is encoded into a binary array of size [800, 800] per sample \n",
    "# Each pixel is 0.1 meter in physiscal space, so 800 * 800 is 80m * 80m centered at the ego car\n",
    "# The ego car is located in the center of the map (400, 400) and it is always facing the left\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "\n",
    "for i, bb in enumerate(target[0]['bounding_box']):\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][i]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
