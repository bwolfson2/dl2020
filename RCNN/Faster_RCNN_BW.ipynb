{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#for image transform\n",
    "import cv2\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "sys.path.append('/root/dl2020')\n",
    "#from model_loader_CP2 import *\n",
    "from CP_helper_RCNN import *\n",
    "from data_helper_RCNN import *\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "#from Unet import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    " \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "#from engine import train_one_epoch, evaluate\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);\n",
    "image_folder = '../data'\n",
    "annotation_csv = '../data/annotation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision\n",
    "# from torchvision.models.detection import FasterRCNN\n",
    "# from torchvision.models.detection.rpn import AnchorGenerator\n",
    "# from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "\n",
    "# # load a pre-trained model for classification and return\n",
    "# # only the features\n",
    "# #backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# backbone = resnet_fpn_backbone('resnet18', pretrained = False)\n",
    "# # FasterRCNN needs to know the number of\n",
    "# # output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# # so we need to add it here\n",
    "# #backbone.out_channels = 1280\n",
    "# backbone.out_channels = 256 #this is a guess\n",
    "\n",
    "\n",
    "# # let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# # location, with 5 different sizes and 3 different aspect\n",
    "# # ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# # map could potentially have different sizes and\n",
    "# # aspect ratios\n",
    "# anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "#                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# # let's define what are the feature maps that we will\n",
    "# # use to perform the region of interest cropping, as well as\n",
    "# # the size of the crop after rescaling.\n",
    "# # if your backbone returns a Tensor, featmap_names is expected to\n",
    "# # be [0]. More generally, the backbone should return an\n",
    "# # OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# # feature maps to use.\n",
    "# roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "#                                                 output_size=7,\n",
    "#                                                 sampling_ratio=2)\n",
    "\n",
    "# # put the pieces together inside a FasterRCNN model\n",
    "# model = FasterRCNN(backbone,\n",
    "#                    num_classes=2,\n",
    "#                    rpn_anchor_generator=anchor_generator,\n",
    "#                    box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "labeled_scene_index = np.arange(106, 134)\n",
    "train_labeled_scene_index, val_labeled_scene_index  = gen_train_val_index(labeled_scene_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = torchvision.transforms.Normalize(mean=[0.6394939, 0.6755114, 0.7049375],\n",
    "                                     std=[0.31936955, 0.3117349 , 0.2953726 ])\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                           normalize\n",
    "                                           ])\n",
    "\n",
    "\n",
    "kwargs = {\n",
    "    #'first_dim': 'sample',\n",
    "    'transform': transform,\n",
    "    'image_folder': image_folder,\n",
    "    'annotation_file': annotation_csv,\n",
    "    'extra_info': True}\n",
    "\n",
    "#dataset_train = LabeledDataset_RCNN (scene_index=train_labeled_scene_index, **kwargs)\n",
    "#dataset_val = LabeledDataset_RCNN (scene_index=val_labeled_scene_index, **kwargs)\n",
    "\n",
    "dataset_train = LabeledDataset(scene_index=train_labeled_scene_index, **kwargs)\n",
    "dataset_val = LabeledDataset(scene_index=val_labeled_scene_index, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=30, shuffle=False, num_workers=4,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=30, shuffle=False, num_workers=4,\n",
    "    collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(img):\n",
    "    plt.imshow(img.numpy().transpose(1, 2, 0))\n",
    "    plt.axis('off');\n",
    "    plt.show()\n",
    "    #fig, ax = plt.subplots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (sample, old_targets, road_image, extra) in enumerate(val_data_loader): #, print_freq, header): \n",
    "        \n",
    "        images = sample\n",
    "        targets = trans_target(old_targets)\n",
    "        #print(\"images len {}, targets len {}\".format(len(images), len(targets)))\n",
    "        #print(\"images[0] shape {}\".format(images[0].shape)) # [6, 3, 256, 306]      \n",
    "        #images = list(image.to(device) for image in images)\n",
    "        #targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 1836])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BEN CHANGED\n",
    "# plot_image(sample[0].reshape(1,3,256,-1))\n",
    "def sew_images(samples):\n",
    "    new_samples = []\n",
    "    for sample in samples:\n",
    "        new_sample =  torch.cat([sample[0],sample[1],sample[2],sample[5],sample[4],sample[3]],axis = 2)\n",
    "        new_samples.append(new_sample)\n",
    "    return new_samples\n",
    "\n",
    "def get_boxes(corners): #this is the corners of the annotaion file\n",
    "    # the corners are in meter and time 10 will convert them in pixels\n",
    "    # Add 400, since the center of the image is at pixel (400, 400)\n",
    "    # The negative sign is because the y axis is reversed for matplotlib\n",
    "    #ax.plot(point_squence.T[0] * 10 + 400, -point_squence.T[1] * 10 + 400, color=color)\n",
    "    \n",
    "    \n",
    "    #['fl_x', 'fr_x', 'bl_x', 'br_x', 'fl_y', 'fr_y','bl_y', 'br_y']\n",
    "    #translate this to boxes to the fastRNN format\n",
    "    xvals = corners[:, :4] *10 +400\n",
    "    yvals = (corners[:, 4:]*10 +400) #not flipping the y vals\n",
    "    \n",
    "    boxes = []\n",
    "    num_obj = corners.shape[0]\n",
    "    #print(corners.shape, num_obj)\n",
    "    for i in range (num_obj):\n",
    "        xmin = np.min(xvals[i])\n",
    "        xmax = np.max(xvals[i])\n",
    "        ymin = np.min(yvals[i])\n",
    "        ymax = np.max(yvals[i])\n",
    "    \n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    \n",
    "    return boxes\n",
    "    \n",
    "n = sew_images(sample)\n",
    "n[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_FastRCNN(model, optimizer, data_loader, device, epoch, print_freq): #this data loader is given loader\n",
    "    \n",
    "    model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    lr_scheduler = None\n",
    "#     if epoch == 0:\n",
    "#         warmup_factor = 1. / 1000\n",
    "#         warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "#         lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "    \n",
    "    #tt = transforms.Compose([transforms.Resize((800, 800)), transforms.ToTensor()]) #this is for 6 images combo\n",
    "    for sample, targets, road_image, extra in metric_logger.log_every(data_loader, print_freq, header): \n",
    "        \n",
    "        #images = sample[0] \n",
    "        \n",
    "        targets = trans_target(old_targets)\n",
    "        #print(\"images len {}, targets len {}\".format(len(images), len(targets)))\n",
    "        #print(\"len(sample) {}, sample [0] shape {}\".format(len(sample), sample[0].shape)) # [6, 3, 256, 306]      \n",
    "        #images = list(image.to(device) for image in images)\n",
    "        images = sew_images(sample)\n",
    "        images = [i.to(device) for i in images]#[(sew_images(s)).to(device) for s in sample] #list of [3, 800, 800], should be 1 per patch\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        #print(loss_dict)\n",
    "        \n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return metric_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_target(old_targets): #target from the given dataset and data loader\n",
    "    tg_list = []\n",
    "    for tg_ in old_targets: #for each item in the batch\n",
    "        target = {}\n",
    "        corners = tg_['bounding_box'].view(-1, 1, 8).squeeze(1).numpy()\n",
    "        boxes = get_boxes(corners)\n",
    "        \n",
    "        categories = tg_['category'].numpy() #switch to array\n",
    "        labels = convert_categories(categories)\n",
    "        masks = gen_masks( corners , labels) \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) #this may need to be rounded but leave for now\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((len(labels)), dtype=torch.int64)\n",
    "        target[\"boxes\"]  = boxes\n",
    "        target[\"labels\"] = labels \n",
    "        target[\"masks\"] = masks\n",
    "        index = 100 #not sure if this matters\n",
    "        target[\"image_id\"] = torch.tensor([index])\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        tg_list.append(target)\n",
    "        \n",
    "    return tuple(tg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(20):\n",
    "#    print(max(torch.min(old_targets[i][\"bounding_box\"]*10+400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.rpn import RPNHead\n",
    "def fr50_Model(pretrained = False):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=pretrained) #true works\n",
    "    # create an anchor_generator for the FPN\n",
    "    # which by default has 5 outputs\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        #sizes=tuple([(16, 32, 64, 128, 256, 512) for _ in range(5)]),\n",
    "        sizes=tuple([(10, 15, 20, 30, 40) for _ in range(5)]),\n",
    "         \n",
    "        aspect_ratios=tuple([(0.25, 0.5, 1.0, 2.0) for _ in range(5)]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.rpn.anchor_generator = anchor_generator\n",
    "\n",
    "    # 256 because that's the number of features that FPN returns\n",
    "    model.rpn.head = RPNHead(256, anchor_generator.num_anchors_per_location()[0])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device (nms_cuda at /tmp/pip-req-build-9d9zypi6/torchvision/csrc/cuda/nms_cuda.cu:127)\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x6d (0x7f120684fe7d in /root/miniconda/envs/gpu/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: nms_cuda(at::Tensor const&, at::Tensor const&, float) + 0x8d1 (0x7f11da3efece in /root/miniconda/envs/gpu/lib/python3.7/site-packages/torchvision/_C.so)\nframe #2: nms(at::Tensor const&, at::Tensor const&, float) + 0x183 (0x7f11da3b3ed7 in /root/miniconda/envs/gpu/lib/python3.7/site-packages/torchvision/_C.so)\nframe #3: <unknown function> + 0x79cf5 (0x7f11da3cdcf5 in /root/miniconda/envs/gpu/lib/python3.7/site-packages/torchvision/_C.so)\nframe #4: <unknown function> + 0x765b0 (0x7f11da3ca5b0 in /root/miniconda/envs/gpu/lib/python3.7/site-packages/torchvision/_C.so)\nframe #5: <unknown function> + 0x70d1e (0x7f11da3c4d1e in /root/miniconda/envs/gpu/lib/python3.7/site-packages/torchvision/_C.so)\nframe #6: <unknown function> + 0x70fc2 (0x7f11da3c4fc2 in /root/miniconda/envs/gpu/lib/python3.7/site-packages/torchvision/_C.so)\nframe #7: <unknown function> + 0x5be4a (0x7f11da3afe4a in /root/miniconda/envs/gpu/lib/python3.7/site-packages/torchvision/_C.so)\nframe #8: _PyMethodDef_RawFastCallKeywords + 0x264 (0x556d949e8ab4 in /root/miniconda/envs/gpu/bin/python)\nframe #9: _PyCFunction_FastCallKeywords + 0x21 (0x556d949e8bd1 in /root/miniconda/envs/gpu/bin/python)\nframe #10: _PyEval_EvalFrameDefault + 0x4ecb (0x556d94a4f57b in /root/miniconda/envs/gpu/bin/python)\nframe #11: _PyFunction_FastCallKeywords + 0xfb (0x556d949e802b in /root/miniconda/envs/gpu/bin/python)\nframe #12: _PyEval_EvalFrameDefault + 0x416 (0x556d94a4aac6 in /root/miniconda/envs/gpu/bin/python)\nframe #13: _PyFunction_FastCallKeywords + 0xfb (0x556d949e802b in /root/miniconda/envs/gpu/bin/python)\nframe #14: _PyEval_EvalFrameDefault + 0x4b39 (0x556d94a4f1e9 in /root/miniconda/envs/gpu/bin/python)\nframe #15: _PyEval_EvalCodeWithName + 0xab8 (0x556d94994b48 in /root/miniconda/envs/gpu/bin/python)\nframe #16: _PyFunction_FastCallKeywords + 0x387 (0x556d949e82b7 in /root/miniconda/envs/gpu/bin/python)\nframe #17: _PyEval_EvalFrameDefault + 0x4b39 (0x556d94a4f1e9 in /root/miniconda/envs/gpu/bin/python)\nframe #18: _PyEval_EvalCodeWithName + 0x2f9 (0x556d94994389 in /root/miniconda/envs/gpu/bin/python)\nframe #19: _PyFunction_FastCallDict + 0x1d5 (0x556d949954c5 in /root/miniconda/envs/gpu/bin/python)\nframe #20: _PyObject_Call_Prepend + 0x63 (0x556d949b4a73 in /root/miniconda/envs/gpu/bin/python)\nframe #21: PyObject_Call + 0x6e (0x556d949a6fde in /root/miniconda/envs/gpu/bin/python)\nframe #22: _PyEval_EvalFrameDefault + 0x1e9d (0x556d94a4c54d in /root/miniconda/envs/gpu/bin/python)\nframe #23: _PyEval_EvalCodeWithName + 0x2f9 (0x556d94994389 in /root/miniconda/envs/gpu/bin/python)\nframe #24: _PyFunction_FastCallDict + 0x1d5 (0x556d949954c5 in /root/miniconda/envs/gpu/bin/python)\nframe #25: _PyObject_Call_Prepend + 0x63 (0x556d949b4a73 in /root/miniconda/envs/gpu/bin/python)\nframe #26: <unknown function> + 0x17d27a (0x556d949fc27a in /root/miniconda/envs/gpu/bin/python)\nframe #27: _PyObject_FastCallKeywords + 0x48b (0x556d949fd2db in /root/miniconda/envs/gpu/bin/python)\nframe #28: _PyEval_EvalFrameDefault + 0x5389 (0x556d94a4fa39 in /root/miniconda/envs/gpu/bin/python)\nframe #29: _PyEval_EvalCodeWithName + 0x2f9 (0x556d94994389 in /root/miniconda/envs/gpu/bin/python)\nframe #30: _PyFunction_FastCallDict + 0x1d5 (0x556d949954c5 in /root/miniconda/envs/gpu/bin/python)\nframe #31: _PyObject_Call_Prepend + 0x63 (0x556d949b4a73 in /root/miniconda/envs/gpu/bin/python)\nframe #32: PyObject_Call + 0x6e (0x556d949a6fde in /root/miniconda/envs/gpu/bin/python)\nframe #33: _PyEval_EvalFrameDefault + 0x1e9d (0x556d94a4c54d in /root/miniconda/envs/gpu/bin/python)\nframe #34: _PyEval_EvalCodeWithName + 0x2f9 (0x556d94994389 in /root/miniconda/envs/gpu/bin/python)\nframe #35: _PyFunction_FastCallDict + 0x1d5 (0x556d949954c5 in /root/miniconda/envs/gpu/bin/python)\nframe #36: _PyObject_Call_Prepend + 0x63 (0x556d949b4a73 in /root/miniconda/envs/gpu/bin/python)\nframe #37: <unknown function> + 0x17d27a (0x556d949fc27a in /root/miniconda/envs/gpu/bin/python)\nframe #38: _PyObject_FastCallKeywords + 0x48b (0x556d949fd2db in /root/miniconda/envs/gpu/bin/python)\nframe #39: _PyEval_EvalFrameDefault + 0x4a96 (0x556d94a4f146 in /root/miniconda/envs/gpu/bin/python)\nframe #40: _PyEval_EvalCodeWithName + 0xab8 (0x556d94994b48 in /root/miniconda/envs/gpu/bin/python)\nframe #41: _PyFunction_FastCallKeywords + 0x387 (0x556d949e82b7 in /root/miniconda/envs/gpu/bin/python)\nframe #42: _PyEval_EvalFrameDefault + 0x14d4 (0x556d94a4bb84 in /root/miniconda/envs/gpu/bin/python)\nframe #43: _PyEval_EvalCodeWithName + 0x2f9 (0x556d94994389 in /root/miniconda/envs/gpu/bin/python)\nframe #44: PyEval_EvalCodeEx + 0x44 (0x556d949952b4 in /root/miniconda/envs/gpu/bin/python)\nframe #45: PyEval_EvalCode + 0x1c (0x556d949952dc in /root/miniconda/envs/gpu/bin/python)\nframe #46: <unknown function> + 0x1db30d (0x556d94a5a30d in /root/miniconda/envs/gpu/bin/python)\nframe #47: _PyMethodDef_RawFastCallKeywords + 0xe9 (0x556d949e8939 in /root/miniconda/envs/gpu/bin/python)\nframe #48: _PyCFunction_FastCallKeywords + 0x21 (0x556d949e8bd1 in /root/miniconda/envs/gpu/bin/python)\nframe #49: _PyEval_EvalFrameDefault + 0x47a4 (0x556d94a4ee54 in /root/miniconda/envs/gpu/bin/python)\nframe #50: _PyGen_Send + 0x2a2 (0x556d949fdf82 in /root/miniconda/envs/gpu/bin/python)\nframe #51: _PyEval_EvalFrameDefault + 0x1a76 (0x556d94a4c126 in /root/miniconda/envs/gpu/bin/python)\nframe #52: _PyGen_Send + 0x2a2 (0x556d949fdf82 in /root/miniconda/envs/gpu/bin/python)\nframe #53: _PyEval_EvalFrameDefault + 0x1a76 (0x556d94a4c126 in /root/miniconda/envs/gpu/bin/python)\nframe #54: _PyGen_Send + 0x2a2 (0x556d949fdf82 in /root/miniconda/envs/gpu/bin/python)\nframe #55: _PyMethodDef_RawFastCallKeywords + 0x8d (0x556d949e88dd in /root/miniconda/envs/gpu/bin/python)\nframe #56: _PyMethodDescr_FastCallKeywords + 0x4f (0x556d949fcdbf in /root/miniconda/envs/gpu/bin/python)\nframe #57: _PyEval_EvalFrameDefault + 0x4c9d (0x556d94a4f34d in /root/miniconda/envs/gpu/bin/python)\nframe #58: _PyFunction_FastCallKeywords + 0xfb (0x556d949e802b in /root/miniconda/envs/gpu/bin/python)\nframe #59: _PyEval_EvalFrameDefault + 0x416 (0x556d94a4aac6 in /root/miniconda/envs/gpu/bin/python)\nframe #60: _PyFunction_FastCallKeywords + 0xfb (0x556d949e802b in /root/miniconda/envs/gpu/bin/python)\nframe #61: _PyEval_EvalFrameDefault + 0x690 (0x556d94a4ad40 in /root/miniconda/envs/gpu/bin/python)\nframe #62: _PyEval_EvalCodeWithName + 0x2f9 (0x556d94994389 in /root/miniconda/envs/gpu/bin/python)\nframe #63: _PyFunction_FastCallDict + 0x3ff (0x556d949956ef in /root/miniconda/envs/gpu/bin/python)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-07de275784a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtrain_one_epoch_FastRCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-6ec14012ad86>\u001b[0m in \u001b[0;36mtrain_one_epoch_FastRCNN\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m#print(loss_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/gpu/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/gpu/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/gpu/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/gpu/lib/python3.7/site-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mproposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_coder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_bbox_deltas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mproposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_proposals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjectness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_anchors_per_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/gpu/lib/python3.7/site-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mfilter_proposals\u001b[0;34m(self, proposals, objectness, image_shapes, num_anchors_per_level)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;31m# non-maximum suppression, independently done per level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0mkeep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatched_nms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnms_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0;31m# keep only topk scoring predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mkeep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_nms_top_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/gpu/lib/python3.7/site-packages/torchvision/ops/boxes.py\u001b[0m in \u001b[0;36mbatched_nms\u001b[0;34m(boxes, scores, idxs, iou_threshold)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0moffsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_coordinate\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mboxes_for_nms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mkeep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes_for_nms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/gpu/lib/python3.7/site-packages/torchvision/ops/boxes.py\u001b[0m in \u001b[0;36mnms\u001b[0;34m(boxes, scores, iou_threshold)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m     32\u001b[0m     \u001b[0m_C\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lazy_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device (nms_cuda at /tmp/pip-req-build-9d9zypi6/torchvision/csrc/cuda/nms_cuda.cu:127)\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x6d (0x7f120684fe7d in /root/miniconda/envs/gpu/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: nms_cuda(at::Tensor const&, at::Tensor const&, float) + 0x8d1 (0x7f11da3efece in /root/miniconda/envs/gpu/lib/python3.7/site-packages/torchvision/_C.so)\nframe #2: nms(at::Tensor const&, at::Tensor const&, float) + 0x183 (0x7f11da3b3ed7 in /root/miniconda/envs/gpu/lib/python3.7/site-packages/torchvision/_C.so)\nframe #3: <unknown function> + 0x79cf5 (0x7f11da3cdcf5 in /root/miniconda/envs/gpu/lib/python3.7/site-packages/torchvision/_C.so)\nframe #4: <unknown function> + 0x765b0 (0x7f11da3ca5b0 in /root/miniconda/envs/gpu/lib/python3.7/site-packages/torchvision/_C.so)\nframe #5: <unknown function> + 0x70d1e (0x7f11da3c4d1e in /root/miniconda/envs/gpu/lib/python3.7/site-packages/torchvision/_C.so)\nframe #6: <unknown function> + 0x70fc2 (0x7f11da3c4fc2 in /root/miniconda/envs/gpu/lib/python3.7/site-packages/torchvision/_C.so)\nframe #7: <unknown function> + 0x5be4a (0x7f11da3afe4a in /root/miniconda/envs/gpu/lib/python3.7/site-packages/torchvision/_C.so)\nframe #8: _PyMethodDef_RawFastCallKeywords + 0x264 (0x556d949e8ab4 in /root/miniconda/envs/gpu/bin/python)\nframe #9: _PyCFunction_FastCallKeywords + 0x21 (0x556d949e8bd1 in /root/miniconda/envs/gpu/bin/python)\nframe #10: _PyEval_EvalFrameDefault + 0x4ecb (0x556d94a4f57b in /root/miniconda/envs/gpu/bin/python)\nframe #11: _PyFunction_FastCallKeywords + 0xfb (0x556d949e802b in /root/miniconda/envs/gpu/bin/python)\nframe #12: _PyEval_EvalFrameDefault + 0x416 (0x556d94a4aac6 in /root/miniconda/envs/gpu/bin/python)\nframe #13: _PyFunction_FastCallKeywords + 0xfb (0x556d949e802b in /root/miniconda/envs/gpu/bin/python)\nframe #14: _PyEval_EvalFrameDefault + 0x4b39 (0x556d94a4f1e9 in /root/miniconda/envs/gpu/bin/python)\nframe #15: _PyEval_EvalCodeWithName + 0xab8 (0x556d94994b48 in /root/miniconda/envs/gpu/bin/python)\nframe #16: _PyFunction_FastCallKeywords + 0x387 (0x556d949e82b7 in /root/miniconda/envs/gpu/bin/python)\nframe #17: _PyEval_EvalFrameDefault + 0x4b39 (0x556d94a4f1e9 in /root/miniconda/envs/gpu/bin/python)\nframe #18: _PyEval_EvalCodeWithName + 0x2f9 (0x556d94994389 in /root/miniconda/envs/gpu/bin/python)\nframe #19: _PyFunction_FastCallDict + 0x1d5 (0x556d949954c5 in /root/miniconda/envs/gpu/bin/python)\nframe #20: _PyObject_Call_Prepend + 0x63 (0x556d949b4a73 in /root/miniconda/envs/gpu/bin/python)\nframe #21: PyObject_Call + 0x6e (0x556d949a6fde in /root/miniconda/envs/gpu/bin/python)\nframe #22: _PyEval_EvalFrameDefault + 0x1e9d (0x556d94a4c54d in /root/miniconda/envs/gpu/bin/python)\nframe #23: _PyEval_EvalCodeWithName + 0x2f9 (0x556d94994389 in /root/miniconda/envs/gpu/bin/python)\nframe #24: _PyFunction_FastCallDict + 0x1d5 (0x556d949954c5 in /root/miniconda/envs/gpu/bin/python)\nframe #25: _PyObject_Call_Prepend + 0x63 (0x556d949b4a73 in /root/miniconda/envs/gpu/bin/python)\nframe #26: <unknown function> + 0x17d27a (0x556d949fc27a in /root/miniconda/envs/gpu/bin/python)\nframe #27: _PyObject_FastCallKeywords + 0x48b (0x556d949fd2db in /root/miniconda/envs/gpu/bin/python)\nframe #28: _PyEval_EvalFrameDefault + 0x5389 (0x556d94a4fa39 in /root/miniconda/envs/gpu/bin/python)\nframe #29: _PyEval_EvalCodeWithName + 0x2f9 (0x556d94994389 in /root/miniconda/envs/gpu/bin/python)\nframe #30: _PyFunction_FastCallDict + 0x1d5 (0x556d949954c5 in /root/miniconda/envs/gpu/bin/python)\nframe #31: _PyObject_Call_Prepend + 0x63 (0x556d949b4a73 in /root/miniconda/envs/gpu/bin/python)\nframe #32: PyObject_Call + 0x6e (0x556d949a6fde in /root/miniconda/envs/gpu/bin/python)\nframe #33: _PyEval_EvalFrameDefault + 0x1e9d (0x556d94a4c54d in /root/miniconda/envs/gpu/bin/python)\nframe #34: _PyEval_EvalCodeWithName + 0x2f9 (0x556d94994389 in /root/miniconda/envs/gpu/bin/python)\nframe #35: _PyFunction_FastCallDict + 0x1d5 (0x556d949954c5 in /root/miniconda/envs/gpu/bin/python)\nframe #36: _PyObject_Call_Prepend + 0x63 (0x556d949b4a73 in /root/miniconda/envs/gpu/bin/python)\nframe #37: <unknown function> + 0x17d27a (0x556d949fc27a in /root/miniconda/envs/gpu/bin/python)\nframe #38: _PyObject_FastCallKeywords + 0x48b (0x556d949fd2db in /root/miniconda/envs/gpu/bin/python)\nframe #39: _PyEval_EvalFrameDefault + 0x4a96 (0x556d94a4f146 in /root/miniconda/envs/gpu/bin/python)\nframe #40: _PyEval_EvalCodeWithName + 0xab8 (0x556d94994b48 in /root/miniconda/envs/gpu/bin/python)\nframe #41: _PyFunction_FastCallKeywords + 0x387 (0x556d949e82b7 in /root/miniconda/envs/gpu/bin/python)\nframe #42: _PyEval_EvalFrameDefault + 0x14d4 (0x556d94a4bb84 in /root/miniconda/envs/gpu/bin/python)\nframe #43: _PyEval_EvalCodeWithName + 0x2f9 (0x556d94994389 in /root/miniconda/envs/gpu/bin/python)\nframe #44: PyEval_EvalCodeEx + 0x44 (0x556d949952b4 in /root/miniconda/envs/gpu/bin/python)\nframe #45: PyEval_EvalCode + 0x1c (0x556d949952dc in /root/miniconda/envs/gpu/bin/python)\nframe #46: <unknown function> + 0x1db30d (0x556d94a5a30d in /root/miniconda/envs/gpu/bin/python)\nframe #47: _PyMethodDef_RawFastCallKeywords + 0xe9 (0x556d949e8939 in /root/miniconda/envs/gpu/bin/python)\nframe #48: _PyCFunction_FastCallKeywords + 0x21 (0x556d949e8bd1 in /root/miniconda/envs/gpu/bin/python)\nframe #49: _PyEval_EvalFrameDefault + 0x47a4 (0x556d94a4ee54 in /root/miniconda/envs/gpu/bin/python)\nframe #50: _PyGen_Send + 0x2a2 (0x556d949fdf82 in /root/miniconda/envs/gpu/bin/python)\nframe #51: _PyEval_EvalFrameDefault + 0x1a76 (0x556d94a4c126 in /root/miniconda/envs/gpu/bin/python)\nframe #52: _PyGen_Send + 0x2a2 (0x556d949fdf82 in /root/miniconda/envs/gpu/bin/python)\nframe #53: _PyEval_EvalFrameDefault + 0x1a76 (0x556d94a4c126 in /root/miniconda/envs/gpu/bin/python)\nframe #54: _PyGen_Send + 0x2a2 (0x556d949fdf82 in /root/miniconda/envs/gpu/bin/python)\nframe #55: _PyMethodDef_RawFastCallKeywords + 0x8d (0x556d949e88dd in /root/miniconda/envs/gpu/bin/python)\nframe #56: _PyMethodDescr_FastCallKeywords + 0x4f (0x556d949fcdbf in /root/miniconda/envs/gpu/bin/python)\nframe #57: _PyEval_EvalFrameDefault + 0x4c9d (0x556d94a4f34d in /root/miniconda/envs/gpu/bin/python)\nframe #58: _PyFunction_FastCallKeywords + 0xfb (0x556d949e802b in /root/miniconda/envs/gpu/bin/python)\nframe #59: _PyEval_EvalFrameDefault + 0x416 (0x556d94a4aac6 in /root/miniconda/envs/gpu/bin/python)\nframe #60: _PyFunction_FastCallKeywords + 0xfb (0x556d949e802b in /root/miniconda/envs/gpu/bin/python)\nframe #61: _PyEval_EvalFrameDefault + 0x690 (0x556d94a4ad40 in /root/miniconda/envs/gpu/bin/python)\nframe #62: _PyEval_EvalCodeWithName + 0x2f9 (0x556d94994389 in /root/miniconda/envs/gpu/bin/python)\nframe #63: _PyFunction_FastCallDict + 0x3ff (0x556d949956ef in /root/miniconda/envs/gpu/bin/python)\n"
     ]
    }
   ],
   "source": [
    "## try training the a model \n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2 #4 returned classifier as nan\n",
    "\n",
    "# get the model using our helper function\n",
    "#model = get_instance_segmentation_model(num_classes)\n",
    "model = fr50_Model(True) #try true first\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0001, #reduce from 0.005 to help with the classifer loss = nan issue\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch_FastRCNN(model, optimizer, train_data_loader, device, epoch, print_freq=100)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset - this has issues\n",
    "    #evaluate(model, val_data_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give me another one\n",
    "tt = transforms.Compose([transforms.Resize((800, 800)), transforms.ToTensor()]) #this is for 6 images combo\n",
    "for i, (sample, old_targets, road_image, extra) in enumerate(val_data_loader): #, print_freq, header): \n",
    "        \n",
    "        images = [tt(sew_images(s)).to(device) for s in sample] #list of [3, 800, 800], should be 1 per patch\n",
    "        targets = trans_target(old_targets)\n",
    "        \n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "     prediction = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([], device='cuda:0', size=(0, 4)),\n",
       "  'labels': tensor([], device='cuda:0', dtype=torch.int64),\n",
       "  'scores': tensor([], device='cuda:0')}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[ 587.7640, -410.7347,  634.5888, -390.4039],\n",
       "          [ 723.5701, -515.9766,  770.2949, -494.8876],\n",
       "          [ 346.5826, -643.8907,  392.2856, -625.2436],\n",
       "          [ 208.3556, -448.8516,  252.5559, -429.6745],\n",
       "          [ 103.3553, -449.4175,  147.5497, -430.6902],\n",
       "          [  31.1752, -483.2730,   79.2614, -463.0602],\n",
       "          [ 340.5685, -484.8769,  384.6458, -465.9312],\n",
       "          [ 648.6622, -573.7671,  696.0050, -555.0985],\n",
       "          [ 496.1005, -604.4378,  541.8034, -585.7907],\n",
       "          [ 519.1313, -483.1805,  561.6338, -464.6352],\n",
       "          [  25.9092, -382.5251,   71.8441, -359.7119],\n",
       "          [ 716.3273, -636.7295,  763.7090, -617.2219],\n",
       "          [ 321.8503, -448.2401,  395.7314, -425.9930],\n",
       "          [ 271.5695, -608.9213,  317.2724, -590.2742],\n",
       "          [ 384.5264, -608.4007,  430.2294, -589.7536],\n",
       "          [  66.1751, -413.8457,  110.3829, -394.0988],\n",
       "          [ 256.3764, -411.0828,  300.9099, -392.4210],\n",
       "          [ 692.8306, -481.0863,  740.1176, -459.0505],\n",
       "          [ 504.9691, -568.7386,  550.6720, -550.0915],\n",
       "          [  16.2913, -451.1110,   60.4900, -432.0538]], device='cuda:0'),\n",
       "  'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         device='cuda:0'),\n",
       "  'masks': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       "  'image_id': tensor([100], device='cuda:0'),\n",
       "  'area': tensor([ 951.9855,  985.3815,  852.2305,  847.6347,  827.6429,  971.9577,\n",
       "           835.0731,  883.8234,  852.2271,  788.2211, 1047.9236,  924.3049,\n",
       "          1643.6449,  852.2271,  852.2277,  872.9662,  831.0745, 1042.0093,\n",
       "           852.2271,  842.3036], device='cuda:0'),\n",
       "  'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         device='cuda:0')}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
