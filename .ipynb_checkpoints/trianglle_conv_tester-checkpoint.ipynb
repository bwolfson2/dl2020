{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7b90a1b150>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CAM_FRONT.jpeg',\n",
       " 'CAM_FRONT_LEFT.jpeg',\n",
       " 'CAM_BACK_LEFT.jpeg',\n",
       " 'CAM_BACK.jpeg',\n",
       " 'CAM_BACK_RIGHT.jpeg',\n",
       " 'CAM_FRONT_RIGHT.jpeg']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FOR TESTNG\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [2, 2]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from helper import collate_fn, draw_box\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = 'data'\n",
    "annotation_csv = 'data/annotation.csv'\n",
    "\n",
    "labeled_scene_index = np.arange(106, 134)\n",
    "image_names\n",
    "\n",
    "def plot_mask(mask):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(mask, cmap='binary');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load data_helper_triangle.py\n",
    "# %load data_helper.py\n",
    "#%%writefile data_helper_triangle.py\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from helper import convert_map_to_lane_map, convert_map_to_road_map\n",
    "from helper import collate_fn, draw_box\n",
    "from shape_splitter import save_masks, get_mask_name\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [2, 2]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "\n",
    "\n",
    "NUM_SAMPLE_PER_SCENE = 126\n",
    "NUM_IMAGE_PER_SAMPLE = 6\n",
    "image_names = [\n",
    "    'CAM_FRONT.jpeg',\n",
    "    'CAM_FRONT_LEFT.jpeg',\n",
    "    'CAM_BACK_LEFT.jpeg',\n",
    "    'CAM_BACK.jpeg',\n",
    "    'CAM_BACK_RIGHT.jpeg',\n",
    "    'CAM_FRONT_RIGHT.jpeg'\n",
    "    ]\n",
    "\n",
    "\n",
    "def plot_mask(mask):\n",
    "    plt.imshow(mask)\n",
    "    plt.axis('off');\n",
    "\n",
    "\n",
    "def load_mask(camera,downsample_shape):\n",
    "    mask_name = get_mask_name(camera,downsample_shape)\n",
    "    if os.path.exists(mask_name):\n",
    "        mask = np.load(mask_name)\n",
    "    else:\n",
    "        save_masks(downsample_shape)\n",
    "        mask = np.load(mask_name)\n",
    "    mask = mask.reshape(downsample_shape).transpose()\n",
    "    return mask\n",
    "    \n",
    "    \n",
    "def rebin(a, shape):\n",
    "    b = a*1.0\n",
    "    sh = shape[0],a.shape[0]//shape[0],shape[1],a.shape[1]//shape[1]\n",
    "    b = b.reshape(sh).mean(-1).mean(1)\n",
    "    return b > 0.5\n",
    "    \n",
    "    \n",
    "#BASICALLY YOU JUST HAVE TO SPECIFY THE IMAGE TO LPAD IN THE __GET_ITEM__ on init\n",
    "# The dataset class for labeled data.\n",
    "class TriangleLabeledDataset(torch.utils.data.Dataset):    \n",
    "    def __init__(self, image_folder, annotation_file, scene_index, transform,extra_info=True,camera='CAM_FRONT.jpeg',downsample_shape=(100,100)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_folder (string): the location of the image folder\n",
    "            annotation_file (string): the location of the annotations\n",
    "            scene_index (list): a list of scene indices for the unlabeled data \n",
    "            transform (Transform): The function to process the image\n",
    "            extra_info (Boolean): whether you want the extra information\n",
    "        \"\"\"\n",
    "        \n",
    "        assert(camera in image_names)\n",
    "        self.camera = camera\n",
    "        self.downsample_shape = downsample_shape\n",
    "        self.mask = load_mask(camera,downsample_shape)\n",
    "        self.image_folder = image_folder\n",
    "        self.annotation_dataframe = pd.read_csv(annotation_file)\n",
    "        self.scene_index = scene_index\n",
    "        self.transform = transform\n",
    "        self.extra_info = extra_info\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.scene_index.size * NUM_SAMPLE_PER_SCENE\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        scene_id = self.scene_index[index // NUM_SAMPLE_PER_SCENE]\n",
    "        sample_id = index % NUM_SAMPLE_PER_SCENE\n",
    "        sample_path = os.path.join(self.image_folder, f'scene_{scene_id}', f'sample_{sample_id}') \n",
    "        \n",
    "        #get camera and only get that image\n",
    "        image_path = os.path.join(sample_path, self.camera)\n",
    "        image = Image.open(image_path)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        data_entries = self.annotation_dataframe[(self.annotation_dataframe['scene'] == scene_id) & (self.annotation_dataframe['sample'] == sample_id)]\n",
    "        corners = data_entries[['fl_x', 'fr_x', 'bl_x', 'br_x', 'fl_y', 'fr_y','bl_y', 'br_y']].to_numpy()\n",
    "        categories = data_entries.category_id.to_numpy()\n",
    "        \n",
    "        ego_path = os.path.join(sample_path, 'ego.png')\n",
    "        ego_image = Image.open(ego_path)\n",
    "        ego_image = torchvision.transforms.functional.to_tensor(ego_image)\n",
    "        road_image = convert_map_to_road_map(ego_image)\n",
    "        road_image_rebinned = rebin(road_image,self.downsample_shape)\n",
    "        road_image_rebinned_mod = torch.Tensor(road_image_rebinned.numpy()*self.mask)\n",
    "        road_image_rebinned_mod = road_image_rebinned[self.mask]\n",
    "        ##Preprocess road image\n",
    "        #road_image = torch.Tensor(road_image.numpy()*self.mask)\n",
    "        ## Get resample down\n",
    "        #road_image_mod = road_image[self.mask]\n",
    "        \n",
    "        target = {}\n",
    "        target['bounding_box'] = torch.as_tensor(corners).view(-1, 2, 4)\n",
    "        target['category'] = torch.as_tensor(categories)\n",
    "\n",
    "        if self.extra_info:\n",
    "            actions = data_entries.action_id.to_numpy()\n",
    "            # You can change the binary_lane to False to get a lane with \n",
    "            lane_image = convert_map_to_lane_map(ego_image, binary_lane=True)\n",
    "            \n",
    "            extra = {}\n",
    "            extra['action'] = torch.as_tensor(actions)\n",
    "            extra['ego_image'] = ego_image\n",
    "            extra['lane_image'] = lane_image\n",
    "\n",
    "            return image, target, road_image_rebinned, extra, road_image_rebinned_mod \n",
    "        \n",
    "        else:\n",
    "            return image, target, road_image_rebinned \n",
    "        \n",
    "\n",
    "def test_loader(camera='CAM_BACK.jpeg'):\n",
    "    transform = torchvision.transforms.ToTensor()\n",
    "    # The labeled dataset can only be retrieved by sample.\n",
    "    # And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "    # You can choose whether the loader returns the extra_info. It is optional. You don't have to use it.\n",
    "    labeled_trainset = TriangleLabeledDataset(image_folder=image_folder,\n",
    "                                      annotation_file=annotation_csv,\n",
    "                                      scene_index=labeled_scene_index,\n",
    "                                      transform=transform,\n",
    "                                      extra_info=True,\n",
    "                                    camera = camera\n",
    "                                     )\n",
    "    trainloader = torch.utils.data.DataLoader(labeled_trainset , batch_size=2, \\\n",
    "                                              shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "    sample, target, road_image_rebinned, extra, road_image_rebinned_mod= iter(trainloader).next()\n",
    "    plt.imshow(sample[0].numpy().transpose(1, 2, 0))\n",
    "    plt.axis('off');\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.imshow(road_image[0], cmap='binary');\n",
    "    plt.show()\n",
    "    plot_mask(labeled_trainset.mask)\n",
    "    return sample, target, road_image_rebinned, extra, road_image_rebinned_mod\n",
    "    \n",
    "if __name__ == \"__main__\":   \n",
    "    image_folder = 'data'\n",
    "    annotation_csv = 'data/annotation.csv'\n",
    "    labeled_scene_index = np.arange(106, 134)\n",
    "    sample, target, road_image_rebinned, extra, road_image_rebinned_mod=test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
