{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470016\n",
      "470016\n"
     ]
    }
   ],
   "source": [
    "# %load CP_helper_RCNN.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "import torchvision.models.detection.mask_rcnn\n",
    "\n",
    "#from coco_utils import get_coco_api_from_dataset\n",
    "#from coco_eval import CocoEvaluator\n",
    "import utils\n",
    "import os\n",
    "\n",
    "\n",
    "def sew_images(sing_samp):\n",
    "        # sing_samp is [6, 3, 256, 306], one item is batch\n",
    "        # output is the image object of all 6 pictures 'sown' together\n",
    "        #############\n",
    "        # A | B | C #\n",
    "        # D | E | F #\n",
    "        #############\n",
    "        \n",
    "        # return [3, 768, 612]\n",
    "        \n",
    "        A1 = sing_samp[0][0]\n",
    "        A2 = sing_samp[0][1]\n",
    "        A3 = sing_samp[0][2]\n",
    "\n",
    "        B1 = sing_samp[1][0]\n",
    "        B2 = sing_samp[1][1]\n",
    "        B3 = sing_samp[1][2]\n",
    "\n",
    "        C1 = sing_samp[2][0]\n",
    "        C2 = sing_samp[2][1]\n",
    "        C3 = sing_samp[2][1]\n",
    "\n",
    "        D1 = sing_samp[3][0]\n",
    "        D2 = sing_samp[3][1]\n",
    "        D3 = sing_samp[3][2]\n",
    "\n",
    "        E1 = sing_samp[4][0]\n",
    "        E2 = sing_samp[4][1]\n",
    "        E3 = sing_samp[4][2]\n",
    "\n",
    "        F1 = sing_samp[5][0]\n",
    "        F2 = sing_samp[5][1]\n",
    "        F3 = sing_samp[5][2]\n",
    "\n",
    "        #print(\"F shape {}\".format(F1.shape))\n",
    "\n",
    "        T1 = torch.cat([A1, B1, C1], 1)\n",
    "        T2 = torch.cat([A2, B2, C2], 1)\n",
    "        T3 = torch.cat([A3, B3, C3], 1)\n",
    "\n",
    "        B1 = torch.cat([D1, E1, F1], 1)\n",
    "        B2 = torch.cat([D2, E2, F2], 1)\n",
    "        B3 = torch.cat([D3, E3, F3], 1)\n",
    "        #print(\"T1 shape {}\".format(T1.shape))\n",
    "\n",
    "        comb1 = torch.cat([T1,B1], 0)\n",
    "        comb2 = torch.cat([T2,B2], 0)\n",
    "        comb3 = torch.cat([T3,B3], 0)\n",
    "\n",
    "        #print(\"comb1 shape {}\".format(comb1.shape)) #should be 768, 612\n",
    "        comb = torch.stack([comb1, comb2, comb3])\n",
    "        toImg = transforms.ToPILImage()\n",
    "        result = toImg(comb) # image object [3, 768, 612]\n",
    "        return result\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.Tensor(size=(6,3,256,306)).random_(0,10)\n",
    "o = sew_images(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 56, 56])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encoder = models.resnet18() #[6, 3, w, h]\n",
    "        self.encoder.fc = Identity() #set last layer to identity, output is [6, 512]\n",
    "        \n",
    "        sing_up = nn.ConvTranspose1d(in_channels = 1, out_channels =1, kernel_size = 5, dilation =2)\n",
    "        self.mult_up = nn.Sequential(*([sing_up]*8)) #this is to get the output from [1, 1, 3072] to [1, 3136]\n",
    "        \n",
    "    def forward(self, x): #x should be [batch, 1, 256, 306] for a single image\n",
    "        output = self.encoder(x)#should be [6, 512]\n",
    "        output = output.view(1, -1) #should be [1, 3072]\n",
    "        output = self.mult_up(output.unsqueeze(0)) #should be [1, 3136]\n",
    "        output = output.view(1, 56, 56)\n",
    "        return output #should be [1, 56, 56]\n",
    "    \n",
    "\n",
    "\n",
    "class UpModel(nn.Module):\n",
    "    \n",
    "    def testfunc():\n",
    "        print(\"hey\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(\"in forward\")\n",
    "        output = self.main(x) \n",
    "        return output\n",
    "        \n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(UpModel, self).__init__()\n",
    "        #self.ngpu = ngpu\n",
    "        self.ngf = 64\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( in_channel, self.ngf * 48, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(self.ngf * 48),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (self.ngf*48) x 4 x 4\n",
    "            nn.ConvTranspose2d(self.ngf * 48, self.ngf * 24, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ngf * 24),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (self.ngf*24) x 8 x 8\n",
    "            nn.ConvTranspose2d( self.ngf * 24, self.ngf * 12, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ngf * 12),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (self.ngf*12) x 16 x 16\n",
    "            nn.ConvTranspose2d( self.ngf * 12, self.ngf * 6, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ngf * 6),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (self.ngf * 6) x 32 x 32\n",
    "            nn.ConvTranspose2d( self.ngf *6 , self.ngf * 3,  4, 2, 1, bias=False),\n",
    "            #nn.Tanh()\n",
    "            nn.BatchNorm2d(self.ngf * 3),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (self.ngf * 3) x 64 x 64\n",
    "    \n",
    "            #from here on just scale up\n",
    "            nn.ConvTranspose2d( self.ngf*3, self.ngf,  5, 3, 0, bias=False), \n",
    "            nn.BatchNorm2d(self.ngf),\n",
    "            nn.ReLU(True),\n",
    "    \n",
    "            nn.ConvTranspose2d( self.ngf, self.ngf,  5, 2, 0, bias=False), \n",
    "            nn.BatchNorm2d(self.ngf),\n",
    "            nn.ReLU(True),\n",
    "        \n",
    "            nn.ConvTranspose2d( self.ngf, self.ngf,  5, 2, 0, bias=False), \n",
    "            nn.BatchNorm2d(self.ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            *([nn.ConvTranspose2d( self.ngf, self.ngf,  5, 1, 0, bias=False), \n",
    "            nn.BatchNorm2d(self.ngf),\n",
    "            nn.ReLU(True)]*3),\n",
    "            \n",
    "            #nn.ConvTranspose2d( self.ngf, self.ngf,  5, 1, 1, bias=False), #add padding to fit into 800 \n",
    "            nn.ConvTranspose2d( self.ngf, out_channel,  6, 1, 1, bias=False),\n",
    "            nn.Tanh())\n",
    "      \n",
    "        \n",
    "\n",
    "class CombModel (nn.Module):\n",
    "      \n",
    "    def get_instance_segmentation_model(self,num_classes, pretrain = False):\n",
    "        # load an instance segmentation model , if pretrain = True, it is pre-trained on COCO\n",
    "        if pretrain:\n",
    "            print(\"in pretrain\")\n",
    "            model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True) #try with pretrained first\n",
    "        else: \n",
    "            model = torchvision.models.detection.maskrcnn_resnet50_fpn()\n",
    "        # get the number of input features for the classifier\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        # replace the pre-trained head with a new one\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "        # now get the number of input features for the mask classifier\n",
    "        in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "        hidden_layer = 256\n",
    "        # and replace the mask predictor with a new one\n",
    "        model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                           hidden_layer,\n",
    "                                                           num_classes)\n",
    "        return model\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CombModel, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = UpModel(3072, 1)\n",
    "        self.maskRCNN = self.get_instance_segmentation_model(num_classes = 2)\n",
    "        \n",
    "    def forward(self, image, target): \n",
    "        #image is tuple([6, 3, 256, 306]), length 1, target is the dictionary of stuff\n",
    "        #target is a tuple if ( dict of boxes, masks etc) length 1\n",
    "        six_encode = self.encoder(image[0]) # output [6, 512]\n",
    "        six_encode = six_encode.view(3072,1,1).unsqueeze(0) #[1, 3072, 1, 1]\n",
    "        print(\"six_encode_shape {}\".format(six_encode.shape))\n",
    "        dec_output = self.decoder(six_encode) #[1, 1, 800, 800]\n",
    "        print(\"decode_output_shape {}\".format(six_encode.shape))\n",
    "           \n",
    "        #output_dict = self.maskRCNN(dec_output)\n",
    "        dec_output = (dec_output.squeeze(0)) #turn it into a tuple of [1, 800, 800] , length 1  \n",
    "        loss_dict = self.maskRCNN(dec_output, target)           \n",
    "              \n",
    "        return loss_dict\n",
    "\n",
    "\n",
    "    \n",
    "def trans_target(old_targets): #target from the given dataset and data loader\n",
    "    tg_list = []\n",
    "    for tg_ in old_targets: #for each item in the batch\n",
    "        target = {}\n",
    "        corners = tg_['bounding_box'].view(-1, 1, 8).squeeze(1).numpy()\n",
    "        boxes = get_boxes(corners)\n",
    "        \n",
    "        categories = tg_['category'].numpy() #switch to array\n",
    "        labels = convert_categories(categories)\n",
    "        masks = gen_masks( corners , labels) \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) #this may need to be rounded but leave for now\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((len(labels)), dtype=torch.int64)\n",
    "        target[\"boxes\"]  = boxes\n",
    "        target[\"labels\"] = labels \n",
    "        target[\"masks\"] = masks\n",
    "        index = 100 #not sure if this matters\n",
    "        target[\"image_id\"] = torch.tensor([index])\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        tg_list.append(target)\n",
    "        \n",
    "    return tuple(tg_list)\n",
    "              \n",
    "\n",
    "              \n",
    "def train_one_epoch_combModel(model, optimizer, data_loader, device, epoch, print_freq): #this data loader is given loader\n",
    "    \n",
    "    model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1. / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "\n",
    "    for sample, old_targets, road_image, extra in metric_logger.log_every(data_loader, print_freq, header): \n",
    "        \n",
    "        images = sample\n",
    "        targets = trans_target(old_targets)\n",
    "        print(\"images len {}, targets len {}\".format(len(images), len(targets)))\n",
    "        print(\"images[0] shape {}\".format(images[0].shape)) # [6, 3, 256, 306]      \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return metric_logger\n",
    "\n",
    "def get_boxes(corners): #this is the corners of the annotaion file\n",
    "    # the corners are in meter and time 10 will convert them in pixels\n",
    "    # Add 400, since the center of the image is at pixel (400, 400)\n",
    "    # The negative sign is because the y axis is reversed for matplotlib\n",
    "    #ax.plot(point_squence.T[0] * 10 + 400, -point_squence.T[1] * 10 + 400, color=color)\n",
    "    \n",
    "    \n",
    "    #['fl_x', 'fr_x', 'bl_x', 'br_x', 'fl_y', 'fr_y','bl_y', 'br_y']\n",
    "    #translate this to boxes to the fastRNN format\n",
    "    xvals = corners[:, :4] *10 +400\n",
    "    yvals = -(corners[:, 4:]*10 +400) #not flipping the y vals\n",
    "    \n",
    "    boxes = []\n",
    "    num_obj = corners.shape[0]\n",
    "    print(corners.shape, num_obj)\n",
    "    for i in range (num_obj):\n",
    "        xmin = np.min(xvals[i])\n",
    "        xmax = np.max(xvals[i])\n",
    "        ymin = np.min(yvals[i])\n",
    "        ymax = np.max(yvals[i])\n",
    "    \n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    \n",
    "    return boxes\n",
    "def convert_categories(categories):\n",
    "    #Old categories\n",
    "     \n",
    "    # 'other_vehicle': 0,\n",
    "    # 'bicycle': 1,\n",
    "    # 'car': 2,\n",
    "    # 'pedestrian': 3,\n",
    "    # 'truck': 4,\n",
    "    # 'bus': 5,\n",
    "    # 'motorcycle': 6,\n",
    "    # 'emergency_vehicle': 7, \n",
    "    # 'animal': 8\n",
    "    \n",
    "    \n",
    "    #New categories\n",
    "     \n",
    "    # 'car': 1,\n",
    "    # 'pedestrian': 2,\n",
    "    # 'all other': 3,\n",
    "     \n",
    "    map_dict = {0:1, 1:1, 2:1, 3:1, 4:1, 5:1, 6:1, 7:1, 8:1}\n",
    "    labels = []\n",
    "    for c in categories:\n",
    "        labels.append(map_dict[c])\n",
    "    return torch.tensor(labels)\n",
    "\n",
    "def gen_masks(corners , labels, img_w = 800, img_h = 800):\n",
    "    '''\n",
    "    essentially fill in the boxes in road_image with the class labels\n",
    "    however all background is 0, hence no road is shown\n",
    "    corners format: ['fl_x', 'fr_x', 'bl_x', 'br_x', 'fl_y', 'fr_y','bl_y', 'br_y']\n",
    "    '''\n",
    "    #print('corners shape {}'.format(corners.shape))\n",
    "    corners = corners*10 +400 #convert into the road image format of 800, 800 with center being 400, 400\n",
    "    xvals = np.round(corners[:, :4], 0).astype(int)\n",
    "    yvals = -np.round(corners[:, 4:], 0).astype(int)\n",
    "    num_obj = len(labels)\n",
    "    #print('num_obj {}'.format(num_obj))\n",
    "    masks = torch.zeros((num_obj, img_w, img_h))\n",
    "    \n",
    "    for i in range(num_obj):\n",
    "        colmin = np.min(xvals[i])\n",
    "        colmax = np.max(xvals[i])\n",
    "        \n",
    "        rowmin = np.min(yvals[i])\n",
    "        rowmax = np.max(yvals[i])\n",
    "        #print(\"mask shape {}\".format(masks.shape))\n",
    "        #print(\"i {}, xmin {}, xmax {}, ymin {}, ymax {} label {}\".format(i, xmin, xmax, ymin, ymax, labels[i]))\n",
    "        masks[i, rowmin:rowmax, colmin:colmax] = labels[i]\n",
    "       \n",
    "    return masks            \n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
