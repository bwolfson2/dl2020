{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#for image transform\n",
    "#import cv2\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "sys.path.insert(1, '//workspace/dl2020')\n",
    "#from model_loader_CP2 import *\n",
    "from CP_helper_RCNN import *\n",
    "from data_helper_RCNN import *\n",
    "from auto_encoder import *\n",
    "\n",
    "#from Unet import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    " \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);\n",
    "image_folder = '../data'\n",
    "annotation_csv = '../data/annotation.csv'\n",
    "\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "labeled_scene_index = np.arange(106, 134)\n",
    "train_labeled_scene_index, val_labeled_scene_index  = gen_train_val_index(labeled_scene_index)\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "kwargs = {\n",
    "    #'first_dim': 'sample',\n",
    "    'transform': transform,\n",
    "    'image_folder': image_folder,\n",
    "    'annotation_file': annotation_csv,\n",
    "    'extra_info': True}\n",
    "\n",
    "#dataset_train = LabeledDataset_RCNN (scene_index=train_labeled_scene_index, **kwargs)\n",
    "#dataset_val = LabeledDataset_RCNN (scene_index=val_labeled_scene_index, **kwargs)\n",
    "\n",
    "dataset_train = LabeledDataset(scene_index=train_labeled_scene_index, **kwargs)\n",
    "dataset_val = LabeledDataset(scene_index=val_labeled_scene_index, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=5, shuffle=False, num_workers=4,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=5, shuffle=False, num_workers=4,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.rpn import RPNHead\n",
    "def fr50_Model(pretrained = False):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=pretrained) #true works\n",
    "    # create an anchor_generator for the FPN\n",
    "    # which by default has 5 outputs\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        #sizes=tuple([(16, 32, 64, 128, 256, 512) for _ in range(5)]),\n",
    "        sizes=tuple([(10, 15, 20, 30, 40) for _ in range(5)]),\n",
    "         \n",
    "        aspect_ratios=tuple([(0.25, 0.5, 1.0, 2.0) for _ in range(5)]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.rpn.anchor_generator = anchor_generator\n",
    "\n",
    "    # 256 because that's the number of features that FPN returns\n",
    "    model.rpn.head = RPNHead(256, anchor_generator.num_anchors_per_location()[0])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#def faster_rcnn_model(pretrained = False):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_params(AE):\n",
    "    if AE.device() != 'cuda':\n",
    "        device = AE.device()\n",
    "        AE.cuda()\n",
    "    for i, (sample, old_targets, road_image, extra) in enumerate(val_data_loader): #, print_freq, header): \n",
    "        if i == 0:\n",
    "            continue\n",
    "        samp_pan = sew_images_panorm(sample) #convert to panoramic tensor\n",
    "        samp_pan_t = torch.stack(samp_pan, dim = 0).cuda() #stack\n",
    "        images = AE.return_image_tensor(samp_pan_t).to('cuda') #list of [3, 800, 800], should be 1 per patch\n",
    "        targets = trans_target(old_targets)\n",
    "        \n",
    "        targets = [{k: v.to('cuda') for k, v in t.items()} for t in targets]\n",
    "        break\n",
    "    AE.device(device)\n",
    "    return images,targets\n",
    "        \n",
    "\n",
    "def get_prediction(images):\n",
    "    with torch.no_grad():\n",
    "         prediction = model(images.cuda())\n",
    "    return prediction\n",
    "\n",
    "def plot_prediction(prediction,targets):\n",
    "\n",
    "    for index in range(prediction.size()[0]):\n",
    "        im = gen_result_chart(prediction[index])\n",
    "        fig, axs = plt.subplots(1,2,figsize=(6, 3))\n",
    "        #color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "\n",
    "        axs[0].imshow(im, cmap ='binary')\n",
    "\n",
    "        #plot target\n",
    "\n",
    "        road_img = gen_result_chart(targets [index])\n",
    "        #fig, ax = plt.subplots(figsize = (2,3))\n",
    "        #color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "\n",
    "        axs[1].imshow(road_img, cmap ='binary')\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#check one sample from train with batch size 20\n",
    "# for i, (sample, old_targets, road_image, extra) in enumerate(train_data_loader): #, print_freq, header): \n",
    "        \n",
    "#         images = sample\n",
    "#         targets = trans_target(old_targets)\n",
    "#         #print(\"images len {}, targets len {}\".format(len(images), len(targets)))\n",
    "#         #print(\"images[0] shape {}\".format(images[0].shape)) # [6, 3, 256, 306]      \n",
    "#         #images = list(image.to(device) for image in images)\n",
    "#         #targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "#         break\n",
    "\n",
    "# normalize = torchvision.transforms.Normalize(mean=[0.6394939, 0.6755114, 0.7049375],\n",
    "#                                  std=[0.31936955, 0.3117349 , 0.2953726 ])\n",
    "\n",
    "# tt = transforms.Compose([transforms.Resize((800, 800)), transforms.ToTensor(), normalize]) \n",
    "\n",
    "# #images = [tt(sew_images(s)) for s in sample] #list of [3, 800, 800], should be 1 per patch\n",
    "# targets = trans_target(old_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvAutoencoder(\n",
       "  (enc_conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 2), padding=(1, 17))\n",
       "  (enc_conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 2), padding=(1, 17))\n",
       "  (enc_conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 2), padding=(1, 15))\n",
       "  (z): ConvTranspose2d(64, 3, kernel_size=(3, 3), stride=(3, 3))\n",
       "  (dec_conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(3, 3))\n",
       "  (dec_conv2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 2), padding=(1, 15))\n",
       "  (dec_conv3): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(1, 2), padding=(1, 17), output_padding=(0, 1))\n",
       "  (dec_conv4): ConvTranspose2d(16, 3, kernel_size=(3, 3), stride=(1, 2), padding=(1, 17), output_padding=(0, 1))\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now check the autoencoder\n",
    "AE = get_autoencoder()\n",
    "AE.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2 #4 returned classifier as nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model using our helper function\n",
    "#model = get_instance_segmentation_model(num_classes)\n",
    "model = fr50_Model(False) #try False\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0001, #reduce from 0.005 to help with the classifer loss = nan issue\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/479]  eta: 0:10:55  lr: 0.000100  loss: 5.2709 (5.2709)  loss_classifier: 4.4972 (4.4972)  loss_box_reg: 0.0064 (0.0064)  loss_objectness: 0.6929 (0.6929)  loss_rpn_box_reg: 0.0744 (0.0744)  time: 1.3691  data: 0.6544  max mem: 7451\n",
      "Epoch: [0]  [100/479]  eta: 0:04:12  lr: 0.000100  loss: 0.9504 (2.1047)  loss_classifier: 0.1415 (1.2874)  loss_box_reg: 0.0008 (0.0061)  loss_objectness: 0.6758 (0.6874)  loss_rpn_box_reg: 0.1233 (0.1237)  time: 0.6834  data: 0.0126  max mem: 7451\n",
      "Epoch: [0]  [200/479]  eta: 0:03:11  lr: 0.000100  loss: 1.0179 (1.5310)  loss_classifier: 0.2328 (0.7209)  loss_box_reg: 0.0251 (0.0072)  loss_objectness: 0.5713 (0.6593)  loss_rpn_box_reg: 0.1674 (0.1436)  time: 0.8104  data: 0.0125  max mem: 7451\n",
      "Epoch: [0]  [300/479]  eta: 0:02:05  lr: 0.000100  loss: 0.9015 (1.3212)  loss_classifier: 0.2019 (0.5425)  loss_box_reg: 0.0378 (0.0140)  loss_objectness: 0.4878 (0.6120)  loss_rpn_box_reg: 0.1604 (0.1527)  time: 0.7030  data: 0.0132  max mem: 7451\n",
      "Epoch: [0]  [400/479]  eta: 0:00:55  lr: 0.000100  loss: 0.8417 (1.1767)  loss_classifier: 0.1956 (0.4441)  loss_box_reg: 0.0357 (0.0153)  loss_objectness: 0.4683 (0.5779)  loss_rpn_box_reg: 0.1588 (0.1393)  time: 0.7422  data: 0.0126  max mem: 7451\n",
      "Epoch: [0]  [478/479]  eta: 0:00:00  lr: 0.000100  loss: 0.9441 (1.1180)  loss_classifier: 0.2021 (0.3976)  loss_box_reg: 0.0243 (0.0166)  loss_objectness: 0.5285 (0.5589)  loss_rpn_box_reg: 0.2022 (0.1448)  time: 0.7605  data: 0.0128  max mem: 7459\n",
      "Epoch: [0] Total time: 0:05:30 (0.6904 s / it)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ConvAutoencoder' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-59bc19935c4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mplot_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-f76b885593dd>\u001b[0m in \u001b[0;36mget_test_params\u001b[0;34m(AE)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_test_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroad_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#, print_freq, header):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/gpu/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 576\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConvAutoencoder' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "train_encoder = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch > 10:\n",
    "        train_encoder = True\n",
    "    if train_encoder:\n",
    "        AE.cuda()\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch_FastRCNN(model, optimizer, train_data_loader, device, epoch, print_freq=100, \n",
    "                             mode = \"autoencode\", encoder = AE,train_encoder = train_encoder )\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    images,target = get_test_params(AE)\n",
    "    predictions = get_prediction(images)\n",
    "    plot_prediction(predictions,target)\n",
    "    if epoch == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConvAutoencoder' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-830640a48a78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-25aaea9f9442>\u001b[0m in \u001b[0;36mget_test_params\u001b[0;34m(AE)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_test_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroad_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#, print_freq, header):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/gpu/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 576\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConvAutoencoder' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "images,target = get_test_params(AE)\n",
    "predictions = get_prediction(images)\n",
    "plot_prediction(predictions,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0ea60749c7c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_image' is not defined"
     ]
    }
   ],
   "source": [
    "plot_image(i[0].detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"temp.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fr50_Model(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"temp.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate on 1 sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate on 1 sample\n",
    "\n",
    "for i, (sample, old_targets, road_image, extra) in enumerate(val_data_loader): #, print_freq, header): \n",
    "        \n",
    "        samp_pan = sew_images_panorm(sample) #convert to panoramic tensor\n",
    "        samp_pan_t = torch.stack(samp_pan, dim = 0) #stack\n",
    "        images = AE.return_image_tensor(samp_pan_t).to(device) #list of [3, 800, 800], should be 1 per patch\n",
    "        targets = trans_target(old_targets)\n",
    "        \n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        if i ==1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.eval()\n",
    "with torch.no_grad():\n",
    "     prediction2 = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot prediction\n",
    " \n",
    "road_img = gen_result_chart(prediction2[0])\n",
    "fig, ax = plt.subplots(figsize=(2, 3))\n",
    "#color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    " \n",
    "ax.imshow(road_img, cmap ='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot target\n",
    " \n",
    "road_img = gen_result_chart(targets [0])\n",
    "fig, ax = plt.subplots(figsize = (2,3))\n",
    "#color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    " \n",
    "ax.imshow(road_img, cmap ='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': num_epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #'loss': loss,\n",
    "            \n",
    "            }, \"../models/fastRCNN_autoencode14epoch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c2a7416a6bb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mto_img\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroad_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "to_img =  transforms.ToPILImage()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i ,(sample, target, road_image, extra) in  enumerate(val_data_loader):\n",
    "\n",
    "        samp_pan = sew_images_panorm(sample) #convert to panoramic tensor\n",
    "        samp_pan_t = torch.stack(samp_pan, dim = 0) #stack\n",
    "        images = AE.return_image_tensor(samp_pan_t).to(device) #list of [3, 800, 800], should be 1 per patch\n",
    "        targets = trans_target(old_targets)\n",
    "\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        prediction_ = model(images)\n",
    "    \n",
    "        road_img = gen_result_chart(prediction_[0])\n",
    "        \n",
    "        sam_img = to_img(samp_pan[0])\n",
    "        \n",
    "        \n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1)\n",
    "        ax1.imshow(sam_img)\n",
    "        ax2.imshow(road_img, cmap ='binary')\n",
    "        ax3.imshow(gen_result_chart(targets [0]), cmap='binary')\n",
    "        \n",
    "        if i == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labeled_scene_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "scene_132DS = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=np.array([132]),\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "s132_loader = torch.utils.data.DataLoader(scene_132DS , batch_size=5, shuffle=False, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scene_132DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i ,(sample, target, road_image, extra) in  enumerate(s132_loader):\n",
    "    print(len(sample))\n",
    "    for b in sample:\n",
    "        plt.imshow(torchvision.utils.make_grid(b, nrow=3).numpy().transpose(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
