{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_nn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_nn.py\n",
    "# %load run_ben_net_down.py\n",
    "import subprocess\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#for image transform\n",
    "import cv2\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "from model_loader_CP2 import *\n",
    "#from CP_helper import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from data_helper_triangle_down import TriangleLabeledDataset,image_names, get_mask_name, load_mask\n",
    "from helper import collate_fn, draw_box\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "def timeStamped(fname, fmt='%Y-%m-%d-%H-%M-%S_{fname}'):\n",
    "    return datetime.datetime.now().strftime(fmt).format(fname=fname)\n",
    "\n",
    "\n",
    "class Identity(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "def gen_train_val_index(labeled_scene_index):\n",
    "    breakpt = len(labeled_scene_index)//3\n",
    "    labeled_scene_index_shuf = labeled_scene_index\n",
    "    random.shuffle(labeled_scene_index_shuf)\n",
    "\n",
    "    train_labeled_scene_index = labeled_scene_index_shuf[:-breakpt]\n",
    "    val_labeled_scene_index = labeled_scene_index_shuf[-breakpt: ]\n",
    "    return train_labeled_scene_index, val_labeled_scene_index\n",
    "\n",
    "def save_file_to_cloud(filename):\n",
    "    current_dir = os.getcwd()\n",
    "    path = os.path.join(current_dir,\"bucket_upload.sh\")\n",
    "    result = subprocess.run(f\"{path} {filename}\",shell=True)\n",
    "    return result\n",
    "\n",
    "def download_file(filename,cloud_filename):\n",
    "    current_dir = os.getcwd()\n",
    "    path = os.path.join(current_dir,\"download_bucket.sh\")\n",
    "    command = f\"{path} {cloud_filename} {filename}\"\n",
    "    print(f\"running {command}\")\n",
    "    if cloud_filename:\n",
    "        subprocess.run(command,shell=True)\n",
    "    print(f\"loading {filename}\")\n",
    "    return filename\n",
    "\n",
    "def save_torch(filename,item,cloud=True):\n",
    "    res = torch.save(item, filename)\n",
    "    if cloud:\n",
    "        res = save_file_to_cloud(filename)\n",
    "    return res\n",
    "\n",
    "def save_cam_model(cam,item,cloud = True):\n",
    "    filename = \"./models/resnet_1\"+cam.replace(\".jpeg\",\".pt\")\n",
    "    save_torch(filename,item,cloud)\n",
    "    fe_filename = \"latest_fe_sd.pt\"\n",
    "    filename = f\"./models/{fe_filename}\"\n",
    "    return save_torch(filename,item['feat_extractor_state_dict'],cloud)\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "def load_object(filename,cloud_filename=None):\n",
    "    if cloud_filename:\n",
    "        print(f\"Downloading {cloud_filename}\")\n",
    "        download_file(filename,cloud_filename)\n",
    "    print(f\"Loading {filename}\")\n",
    "    return torch.load(filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_output_layer_size(cam,downsample_shape):\n",
    "    return load_mask(get_mask_name(cam,downsample_shape),downsample_shape).sum()\n",
    "\n",
    "def get_output_layer(input_size,output_layer_size):\n",
    "    return nn.Linear(input_size,output_layer_size)\n",
    "\n",
    "def create_blank_model(base_model,last_layer_size,output_layer_size):\n",
    "    base_model = base_model()\n",
    "    base_model.fc = Identity()\n",
    "    output_layer = get_output_layer(last_layer_size,output_layer_size)\n",
    "    return nn.Sequential(base_model,output_layer,nn.Sigmoid())\n",
    "\n",
    "def get_feature_extractor(base_model):\n",
    "    model = base_model()\n",
    "    model.fc = Identity()\n",
    "    return model\n",
    "\n",
    "def load_model_with_state_dicts(base_model,feature_extractor_sd,output_layer_sd):\n",
    "    fe = get_feature_extractor(base_model)\n",
    "    fe.load_state_dict(feature_extractor_sd)\n",
    "    output_layer = get_output_layer(output_layer_sd[\"weight\"][0].size()[0],\\\n",
    "                                    output_layer_size=output_layer_sd[\"bias\"].size()[0])\n",
    "    output_layer.load_state_dict(output_layer_sd)\n",
    "    return nn.Sequential(fe,output_layer,nn.Sigmoid())\n",
    "\n",
    "\n",
    "def create_model_with_feature_extractor(feature_extractor,last_layer_size,output_layer_size):\n",
    "    feature_extractor.fc = Identity()\n",
    "    output_layer = get_output_layer(last_layer_size,output_layer_size)\n",
    "    return nn.Sequential(feature_extractor,output_layer,nn.Sigmoid())\n",
    "\n",
    "\n",
    "def load_cam_model(cam,latest_fe = True,cloud = True,requires_grad=True):\n",
    "    cam_short = cam.replace('jpeg','pt')\n",
    "    cloud_filename = f\"resnet_1{cam_short}\"\n",
    "    filename = f\"./models/{cloud_filename}\"\n",
    "    if cloud:\n",
    "        out_sd = load_object(filename,cloud_filename)\n",
    "    else:\n",
    "        out_sd = load_object(filename)\n",
    "    out_sdr = out_sd[\"output_layer_state_dict\"]\n",
    "    if latest_fe:\n",
    "        cloud_filename = \"latest_fe_sd.pt\"\n",
    "        filename = f\"./models/{cloud_filename}\"\n",
    "        if cloud:\n",
    "            fe_sd = load_object(filename,cloud_filename)\n",
    "        else:\n",
    "            fe_sd = load_object(filename)\n",
    "    else:\n",
    "        fe_sd = out_sd[\"feat_extractor_state_dict\"]\n",
    "    \n",
    "    model = load_model_with_state_dicts(models.resnet18,fe_sd,out_sdr)\n",
    "    if not requires_grad:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad=False\n",
    "    return model\n",
    "\n",
    "\n",
    "def degrad_layers(model,layers):\n",
    "    for layer in layers:\n",
    "        for param in model[layer].parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "def grad_layers(model,layers):\n",
    "    for layer in layers:\n",
    "        for param in model[layer].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "     \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i ,(sample, target, road_image, extra, road_image_mod) in enumerate(loader):\n",
    "             \n",
    "            sample_ = torch.stack(sample,0).cuda() #should be [batch size,3, h,w]\n",
    "            \n",
    "            labels = torch.stack(road_image_mod, 0).cuda()\n",
    "            \n",
    "            \n",
    "            outputs = model(sample_)\n",
    "            predicted = (outputs>0.5).int() ## convert to bineary\n",
    "            \n",
    "            total += (labels.size(0)*labels.size(1))\n",
    "            correct += predicted.eq(labels.int()).sum().item()\n",
    "        \n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(feat_extractor, **train_kwargs):\n",
    "    #save model\n",
    "    if not os.path.exists(\"./models\"):\n",
    "        os.mkdir(\"models\")\n",
    "    \n",
    "    print(\"args to train function:\")\n",
    "    print(train_kwargs)\n",
    "    \n",
    "    for cycle in range(train_kwargs[\"train_cycles\"]):\n",
    "        for cam in image_names: #let's try just front camera\n",
    "            print(\"training {}\".format(cam))\n",
    "            #make camera specific train loader\n",
    "            labeled_trainset = training_tools[cam][1]\n",
    "            train_loader = torch.utils.data.DataLoader(labeled_trainset , batch_size=train_kwargs[\"batch\"], \n",
    "                                                      shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "            labeled_valset = training_tools[cam][2]\n",
    "            val_loader = torch.utils.data.DataLoader(labeled_valset , batch_size=train_kwargs[\"batch\"], \n",
    "                                                      shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "            if train_kwargs.get(\"load_models\",False): \n",
    "                model = load_cam_model(cam,latest_fe=True,cloud=train_kwargs[\"load_cloud\"]).cuda()\n",
    "                \n",
    "            else:\n",
    "                output_layer = training_tools[cam][0] #output the layer\n",
    "\n",
    "\n",
    "                #make camera spcific model\n",
    "                model = nn.Sequential(feat_extractor, output_layer, nn.Sigmoid()).cuda()\n",
    "\n",
    "\n",
    "            criterion = torch.nn.BCELoss(reduction = 'sum') #trying summation\n",
    "            train_losses = []\n",
    "            val_accs = []\n",
    "\n",
    "            for e in range(train_kwargs[\"epochs\"]):\n",
    "                print(f\"epoch: {e}\")\n",
    "                t = time.process_time()\n",
    "                if e < train_kwargs[\"eto\"]:\n",
    "                    print(\"training output layer\")\n",
    "                    degrad_layers(model,[0]) #degrad the base model\n",
    "                else:\n",
    "                    print(\"training whole network\")\n",
    "                    grad_layers(model,[0])\n",
    "\n",
    "                param_list = [p for p in model.parameters() if p.requires_grad]\n",
    "                optimizer = torch.optim.Adam(param_list, lr=train_kwargs[\"lr\"], eps=train_kwargs[\"eps\"])\n",
    "\n",
    "                for i ,(sample, target, road_image, extra, road_image_mod) in enumerate(train_loader):\n",
    "\n",
    "                    sample_ = torch.stack(sample,0).cuda() #should be [batch size,3, h,w]\n",
    "                    labels = torch.stack(road_image_mod, 0).cuda() #should be [batch size, cropsize]\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(sample_) \n",
    "\n",
    "                    loss = criterion(outputs, labels.float())\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    train_losses.append(loss.item())\n",
    "\n",
    "                # validate every 200 iterations\n",
    "                val_acc = test_model(val_loader, model) #calls model.eval()\n",
    "                val_accs.append(val_acc)\n",
    "                #do some stuff\n",
    "                elapsed_time = time.process_time() - t\n",
    "                print('Epoch: [{}], Step: [{}], Train Loss {:.4f}, Validation Acc: {:.4f}, time {:.4f}'.format( \n",
    "                           e+1, i+1, loss,  val_acc, elapsed_time))\n",
    "                model.train() #go back to training\n",
    "                t = time.process_time()\n",
    "\n",
    "            print(\"save camera model\") \n",
    "            item = {\n",
    "\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'feat_extractor_state_dict':  feat_extractor.state_dict(),\n",
    "                'output_layer_state_dict': model[1].state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_accs': val_accs\n",
    "                }\n",
    "            save_cam_model(cam,item,cloud=train_kwargs[\"save_cloud\"])\n",
    "            if cycle == (train_kwargs[\"train_cycles\"] -1 ) and\\\n",
    "            train_kwargs[\"last_save\"] and\\\n",
    "            not train_kwargs[\"save_cloud\"]:\n",
    "                save_cam_model(cam,item,cloud=True)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Run neural net, first argument is downsampling rate')\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--d\", help=\"The downsample size for the image (1 dimension)\",default=100,\n",
    "                        type=int)\n",
    "    parser.add_argument(\"--b\", help=\"batch-size\",default=100,\n",
    "                        type=int)\n",
    "    parser.add_argument(\"--e\", help=\"epochs\",default=5,\n",
    "                    type=int)\n",
    "    parser.add_argument(\"--s\", help=\"save to cloud? default no\",default=0,\n",
    "                    type=int)\n",
    "    parser.add_argument(\"--l\", help=\"load from cloud? default no\",default=0,\n",
    "                    type=int)    \n",
    "    parser.add_argument(\"--tc\", help=\"training_cycles\",default=1,\n",
    "                    type=int)    \n",
    "    parser.add_argument(\"--ls\",help='save last cycle',default=1,type=int)\n",
    "    parser.add_argument(\"--eto\",help='how many epochs to train output layer alone',default=0,type=int)\n",
    "    args = parser.parse_args()\n",
    "    downsample_shape = (args.d,args.d)\n",
    "    \n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0);\n",
    "\n",
    "\n",
    "    image_folder = 'data'\n",
    "    annotation_csv = 'data/annotation.csv'\n",
    "\n",
    "    \n",
    "    unlabeled_scene_index = np.arange(106)\n",
    "    labeled_scene_index = np.arange(106, 134)\n",
    "    \n",
    "    \n",
    "    normalize = torchvision.transforms.Normalize(mean=[0.6394939, 0.6755114, 0.7049375],\n",
    "                                         std=[0.31936955, 0.3117349 , 0.2953726 ])\n",
    "\n",
    "    transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                               normalize\n",
    "                                               ])\n",
    "\n",
    "    train_labeled_scene_index, val_labeled_scene_index = gen_train_val_index(labeled_scene_index)\n",
    "    crop_size = {cam:get_output_layer_size(cam,downsample_shape) for cam in image_names}\n",
    "    print(crop_size)\n",
    "    training_tools = {cam: (nn.Linear(512, crop_size[cam]), \n",
    "                           #training set\n",
    "                           TriangleLabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True,\n",
    "                                camera = cam,downsample_shape=downsample_shape),\n",
    "                           #validation set\n",
    "                            TriangleLabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True,\n",
    "                                camera = cam,downsample_shape=downsample_shape),\n",
    "                       \n",
    "                       \n",
    "                       ) for cam in image_names}\n",
    "    \n",
    "    feat_extractor = torchvision.models.resnet18()\n",
    "    feat_extractor.fc = Identity() #change it to identity\n",
    "\n",
    "    train_kwargs={\n",
    "        'epochs':args.e,\n",
    "        'lr': 2e-05,\n",
    "        'momentum': 0.99,\n",
    "        'eps':1e-08,\n",
    "        'batch':args.b,\n",
    "        'load_models':True,\n",
    "        'load_cloud': args.l,\n",
    "        'save_cloud':args.s,\n",
    "        'train_cycles':args.tc,\n",
    "        'lsat_save':args.ls,\n",
    "        'eto':args.eto\n",
    "        }\n",
    "    \n",
    "    \n",
    "    train(feat_extractor, **train_kwargs)\n",
    "    \n",
    "    print('finished')\n",
    "\n",
    "#sample command\n",
    "#python test_nn.py --d 100 --b 100 --e 5 --s 0 --l 0 --tc 10 --ls 1 --eto 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 and 10 and not 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./models/resnet_1CAM_FRONT.pt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1c2658d7a830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#     val_acc = test_model(val_loader, model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"accuracy on {cam} is {val_acc}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'val_acc' is not defined"
     ]
    }
   ],
   "source": [
    "image_folder = 'data'\n",
    "annotation_csv = 'data/annotation.csv'\n",
    "\n",
    "\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "labeled_scene_index = np.arange(106, 134)\n",
    "\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean=[0.6394939, 0.6755114, 0.7049375],\n",
    "                                     std=[0.31936955, 0.3117349 , 0.2953726 ])\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                           normalize\n",
    "                                           ])\n",
    "\n",
    "train_labeled_scene_index, val_labeled_scene_index = gen_train_val_index(labeled_scene_index)\n",
    "\n",
    "\n",
    "for cam in image_names:\n",
    "    \n",
    "    downsample_shape = (100,100)\n",
    "\n",
    "    labeled_valset = TriangleLabeledDataset(image_folder=image_folder,\n",
    "          annotation_file=annotation_csv,\n",
    "          scene_index=val_labeled_scene_index,\n",
    "          transform=transform,\n",
    "          extra_info=True,\n",
    "        camera = cam,downsample_shape=downsample_shape)\n",
    "\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(labeled_valset , batch_size=100, \n",
    "                                              shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "    model = load_cam_model(cam,False,False).cuda()\n",
    "\n",
    "    val_acc = test_model(val_loader, model)\n",
    "\n",
    "    print(f\"accuracy on {cam} is {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./models/resnet_1CAM_FRONT.pt\n"
     ]
    }
   ],
   "source": [
    "model = load_cam_model(cam,False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degrad_layers(model,[0])\n",
    "param_list = [p for p in model.parameters() if p.requires_grad]\n",
    "param_list\n",
    "[i.requires_grad for i in model[1].parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.requires_grad for i in model[0].parameters()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
