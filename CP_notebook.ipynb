{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256-128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128+64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#for image transform\n",
    "import cv2\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "from model_loader_CP2 import *\n",
    "#from CP_helper import *\n",
    "from Unet import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.93 ms\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 636 ms\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 600 µs\n"
     ]
    }
   ],
   "source": [
    "image_folder = 'data'\n",
    "annotation_csv = 'data/annotation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 743 µs\n"
     ]
    }
   ],
   "source": [
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "labeled_scene_index = np.arange(106, 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118,\n",
       "       119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131,\n",
       "       132, 133])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.9 ms\n"
     ]
    }
   ],
   "source": [
    "labeled_scene_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.52 ms\n"
     ]
    }
   ],
   "source": [
    "len(labeled_scene_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 657 µs\n"
     ]
    }
   ],
   "source": [
    "labeled_scene_index_shuf = labeled_scene_index\n",
    "random.shuffle(labeled_scene_index_shuf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([109, 120, 116, 123, 111, 106, 113, 127, 125, 124, 129, 131, 110,\n",
       "       108, 128, 112, 117, 126, 115, 132, 121, 122, 114, 107, 119, 130,\n",
       "       118, 133])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.77 ms\n"
     ]
    }
   ],
   "source": [
    "labeled_scene_index_shuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 683 µs\n"
     ]
    }
   ],
   "source": [
    "train_labeled_scene_index = labeled_scene_index_shuf[:-10]\n",
    "val_labeled_scene_index = labeled_scene_index_shuf[-10: 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([109, 120, 116, 123, 111, 106, 113, 127, 125, 124, 129, 131, 110,\n",
       "       108, 128, 112, 117, 126])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.75 ms\n"
     ]
    }
   ],
   "source": [
    "train_labeled_scene_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.44 ms\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    #'first_dim': 'sample',\n",
    "    'transform': transforms.ToTensor(),\n",
    "    'image_folder': image_folder,\n",
    "    'annotation_file': annotation_csv,\n",
    "    'extra_info': True}\n",
    "\n",
    "def gen_train_val_loader(labeled_scene_index, **kwargs):\n",
    "    labeled_scene_index_shuf = labeled_scene_index\n",
    "    random.shuffle(labeled_scene_index_shuf)\n",
    "    train_labeled_scene_index = labeled_scene_index_shuf[:-10] #hard code we know there are only 28 scenes that are labeled\n",
    "    val_labeled_scene_index = labeled_scene_index_shuf[-10:]\n",
    "    \n",
    "    print(len(train_labeled_scene_index), len(val_labeled_scene_index))\n",
    "    print(train_labeled_scene_index[0], val_labeled_scene_index[0])\n",
    "        \n",
    "    loadkwargs = {'batch_size': 2,\n",
    "    'shuffle': True,\n",
    "    'collate_fn':collate_fn,\n",
    "    'num_workers':2,\n",
    "    \n",
    "    }\n",
    "    \n",
    "    labeled_trainset = LabeledDataset(scene_index=train_labeled_scene_index, **kwargs)\n",
    "    print(len(labeled_trainset))\n",
    "    trainloader = torch.utils.data.DataLoader(labeled_trainset, **loadkwargs)\n",
    "    \n",
    "    labeled_valset = LabeledDataset(scene_index=val_labeled_scene_index, **kwargs)  \n",
    "    print(len(labeled_valset))\n",
    "    valloader = torch.utils.data.DataLoader(labeled_valset, **loadkwargs)\n",
    "    \n",
    "    result={\"train\" : trainloader,\n",
    "           \"val\": valloader\n",
    "        \n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 10\n",
      "110 117\n",
      "2268\n",
      "1260\n",
      "time: 255 ms\n"
     ]
    }
   ],
   "source": [
    "train_val_loader = gen_train_val_loader(labeled_scene_index, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([6, 3, 256, 306])\n",
      "torch.Size([800, 800])\n",
      "time: 279 ms\n"
     ]
    }
   ],
   "source": [
    "train_loader = train_val_loader[\"train\"]\n",
    "for i ,(sample, target, road_image, extra) in enumerate(train_loader):\n",
    "    print(len(sample))\n",
    "    print(sample[0].shape)\n",
    "    print(road_image[0].shape)\n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.15 s\n"
     ]
    }
   ],
   "source": [
    "##spot check\n",
    "sample_ = ModelLoader.get_binary_road_map(sample).cuda() #should be [batch size, 800, 800]\n",
    "labels = torch.stack(road_image, 0).cuda() #should be [batch size, 800, 800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 800, 800])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.02 ms\n"
     ]
    }
   ],
   "source": [
    "sample_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(429651, device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.42 ms\n"
     ]
    }
   ],
   "source": [
    "labels.int().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(699921, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.99 ms\n"
     ]
    }
   ],
   "source": [
    "(sample_>0.5).int().sum()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 800, 800])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.74 ms\n"
     ]
    }
   ],
   "source": [
    "(sample_>0.5).int().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 800, 800])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.82 ms\n"
     ]
    }
   ],
   "source": [
    "labels.int().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "630356"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.17 ms\n"
     ]
    }
   ],
   "source": [
    "(sample_>0.5).int().eq(labels.int()).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.492465625"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.52 ms\n"
     ]
    }
   ],
   "source": [
    "630356/1280000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 75 ms\n"
     ]
    }
   ],
   "source": [
    "testUnet = UNet(in_channel=1,out_channel=1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottleneck1 torch.Size([2, 256, 184, 184]),  encode_block3torch.Size([2, 256, 193, 193])\n",
      "decode_block3torch.Size([2, 512, 184, 184])\n",
      "cat_layer2 torch.Size([2, 128, 360, 360]),  decode_block2torch.Size([2, 128, 394, 394])\n",
      "decode_block2torch.Size([2, 256, 360, 360])\n",
      "cat_layer1 shape torch.Size([2, 64, 712, 712]), encode_bl1 shapetorch.Size([2, 64, 796, 796])\n",
      "pre final shape torch.Size([2, 128, 712, 712])\n",
      "pre refit layer shape torch.Size([2, 1, 708, 708])\n",
      "time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "output1 = testUnet(sample_.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 800, 800])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.77 ms\n"
     ]
    }
   ],
   "source": [
    "output1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 733 µs\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.BCELoss(reduction = \"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 25.2 ms\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(output1.squeeze(1), labels.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(611754.4375, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.32 ms\n"
     ]
    }
   ],
   "source": [
    "output1.squeeze(1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7723, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.33 ms\n"
     ]
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 684 µs\n"
     ]
    }
   ],
   "source": [
    "criterion2 = torch.nn.BCELoss(reduction = \"sum\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.06 ms\n"
     ]
    }
   ],
   "source": [
    "loss2 = criterion(output1.squeeze(1), labels.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7723, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.51 ms\n"
     ]
    }
   ],
   "source": [
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1280000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.9 ms\n"
     ]
    }
   ],
   "source": [
    "labels.view(1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1280000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.96 ms\n"
     ]
    }
   ],
   "source": [
    "output1.view(1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, device='cuda:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.49 ms\n"
     ]
    }
   ],
   "source": [
    "output1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sjafk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-b3e2e59af072>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msjafk\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mdsa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sjafk' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 117 ms\n"
     ]
    }
   ],
   "source": [
    "sjafk;dsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs={\n",
    "    'epochs':1,\n",
    "    \"lr\": 0.01,\n",
    "    'momentum': 0.99\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "     \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i ,(sample, target, road_image, extra) in enumerate(loader):\n",
    "             \n",
    "            sample_ = ModelLoader.get_binary_road_map(sample).cuda() #should be [batch size, 800, 800]\n",
    "            labels = torch.stack(road_image, 0).cuda() #should be [batch size, 800, 800]\n",
    "            \n",
    "            \n",
    "            outputs = model(sample_.unsqueeze(1))\n",
    "            outputs = outputs.squeeze(1)\n",
    "            predicted = (outputs>0.5).int() ## convert to bineary\n",
    "            \n",
    "            total += (labels.size(0)*labels.size(1)*labels.size(2))\n",
    "            correct += predicted.eq(labels.int()).sum().item()\n",
    "        \n",
    "    return (100 * correct / total)\n",
    "     \n",
    "\n",
    "\n",
    "def train(train_val_loader, **train_kwargs):\n",
    "    #initialize stuff...\n",
    "    train_loader = train_val_loader[\"train\"]\n",
    "    val_loader = train_val_loader[\"val\"]\n",
    "    \n",
    "    unet = UNet(in_channel=1,out_channel=1).cuda()\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    param_list = [p for p in unet.parameters() if p.requires_grad]\n",
    "    print(len(praam_list))\n",
    "    optimizer = torch.optim.SGD(param_list, lr = train_kwargs[\"lr\"], momentum=train_kwargs[\"momentum\"])\n",
    "    train_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    unet.train()\n",
    "    for e in range(train_kwargs[\"epochs\"]):\n",
    "        t = time.process_time()\n",
    "\n",
    "        for i ,(sample, target, road_image, extra) in enumerate(train_loader):\n",
    "            sample_ = ModelLoader.get_binary_road_map(sample).cuda() #should be [batch size, 800, 800]\n",
    "            labels = torch.stack(road_image, 0).cuda() #should be [batch size, 800, 800]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = unet(sample_.unsqueeze(1)) #unet needs the channels dimension\n",
    "            outputs = outputs.squeeze(1)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # validate every 200 iterations\n",
    "            if i > 0 and i % 100== 0:\n",
    "                val_acc = test_model(val_loader, unet) #calls model.eval()\n",
    "                val_accs.append(val_acc)\n",
    "                #do some stuff\n",
    "                elapsed_time = time.process_time() - t\n",
    "                print('Epoch: [{}], Step: [{}], Train Loss {:.4f}, Validation Acc: {:.4f}, time {:.4f}'.format( \n",
    "                           e+1, i+1, loss,  val_acc, elapsed_time))\n",
    "                unet.train() #go back to training\n",
    "                t = time.process_time()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs={\n",
    "    'epochs':1,\n",
    "    \"lr\": 0.01,\n",
    "    'momentum': 0.99\n",
    "    }\n",
    "\n",
    "train(train_val_loader, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfafdsafdsafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sample\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "unlabeled_trainset = UnlabeledDataset(image_folder=image_folder,scene_index=unlabeled_scene_index, first_dim='sample', transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(unlabeled_trainset, batch_size=3, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [batch_size, 6(images per sample), 3, H, W]\n",
    "sample = iter(trainloader).next()\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 6 images orgenized in the following order:\n",
    "# 0 CAM_FRONT_LEFT, 1 CAM_FRONT, 2 CAM_FRONT_RIGHT, 3 CAM_BACK_LEFT, 4 CAM_BACK, 5 CAM_BACK_RIGHT\n",
    "plt.imshow(torchvision.utils.make_grid(sample[2], nrow=3).numpy().transpose(1, 2, 0)) #need the transpose\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "# You can choose whether the loader returns the extra_info. It is optional. You don't have to use it.\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "LB_trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target, road_image, extra = iter(LB_trainloader).next()\n",
    "print(torch.stack(sample).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toImg = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "toImg(sample[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torchvision.utils.make_grid(sample[0], nrow=3).numpy().transpose(1, 2, 0)) #need the transpose\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROAD IMAGE ##\n",
    "# The road map layout is encoded into a binary array of size [800, 800] per sample \n",
    "# Each pixel is 0.1 meter in physiscal space, so 800 * 800 is 80m * 80m centered at the ego car\n",
    "# The ego car is located in the center of the map (400, 400) and it is always facing the left\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "\n",
    "for i, bb in enumerate(target[0]['bounding_box']):\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][i]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_image[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ = ModelLoader.get_binary_road_map(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min max is between 0 and 1\n",
    "print(sample_[0].min())\n",
    "print(sample_[1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "resnet18_pre = models.resnet18(pretrained=True)\n",
    "resnet18_raw = models.resnet18(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a normalize tranform \n",
    "# res_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet requires all sizes to be 224 by 224\n",
    "# res_transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_img = ModelLoader.sew_images(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_ftrs = resnet18_pre.fc.in_features\n",
    "# resnet18_pre.fc = Identity() #first set it to identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##resnet is only for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_.unsqueeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#afjkd;a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try one sample of for Unet\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "\n",
    "sing_samp_unet = sample_.unsqueeze(1)\n",
    "testUnet = UNet(in_channels, out_channels)\n",
    "testUnetout = testUnet(sing_samp_unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testUnetout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testUnetout.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(road_image[:2],0).unsqueeze(1).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((testUnetout>0.5).int()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(road_image[:2],0).unsqueeze(1).eq((testUnetout>0.5).int()).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "649756/(2*800*800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure out loss Use BCE loss\n",
    "\n",
    "\n",
    "sample, target, road_image, extra = iter(LB_trainloader).next()\n",
    "\n",
    "## ROAD IMAGE ##\n",
    "# The road map layout is encoded into a binary array of size [800, 800] per sample \n",
    "# Each pixel is 0.1 meter in physiscal space, so 800 * 800 is 80m * 80m centered at the ego car\n",
    "# The ego car is located in the center of the map (400, 400) and it is always facing the left\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "\n",
    "for i, bb in enumerate(target[0]['bounding_box']):\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][i]])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure out loss Use BCE loss\n",
    "\n",
    "\n",
    "sample, target, road_image, extra = iter(LB_trainloader).next()\n",
    "\n",
    "## ROAD IMAGE ##\n",
    "# The road map layout is encoded into a binary array of size [800, 800] per sample \n",
    "# Each pixel is 0.1 meter in physiscal space, so 800 * 800 is 80m * 80m centered at the ego car\n",
    "# The ego car is located in the center of the map (400, 400) and it is always facing the left\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "\n",
    "for i, bb in enumerate(target[0]['bounding_box']):\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][i]])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sample, target, road_image, extra = iter(LB_trainloader).next()\n",
    "\n",
    "## ROAD IMAGE ##\n",
    "# The road map layout is encoded into a binary array of size [800, 800] per sample \n",
    "# Each pixel is 0.1 meter in physiscal space, so 800 * 800 is 80m * 80m centered at the ego car\n",
    "# The ego car is located in the center of the map (400, 400) and it is always facing the left\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "\n",
    "for i, bb in enumerate(target[0]['bounding_box']):\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][i]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
