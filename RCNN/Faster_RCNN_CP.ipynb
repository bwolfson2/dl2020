{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#for image transform\n",
    "import cv2\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "sys.path.insert(1, '//workspace/dl2020')\n",
    "#from model_loader_CP2 import *\n",
    "from CP_helper_RCNN import *\n",
    "from data_helper_RCNN import *\n",
    "\n",
    "#from Unet import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    " \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "#from engine import train_one_epoch, evaluate\n",
    "\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);\n",
    "image_folder = '../data'\n",
    "annotation_csv = '../data/annotation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision\n",
    "# from torchvision.models.detection import FasterRCNN\n",
    "# from torchvision.models.detection.rpn import AnchorGenerator\n",
    "# from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "\n",
    "# # load a pre-trained model for classification and return\n",
    "# # only the features\n",
    "# #backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# backbone = resnet_fpn_backbone('resnet18', pretrained = False)\n",
    "# # FasterRCNN needs to know the number of\n",
    "# # output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# # so we need to add it here\n",
    "# #backbone.out_channels = 1280\n",
    "# backbone.out_channels = 256 #this is a guess\n",
    "\n",
    "\n",
    "# # let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# # location, with 5 different sizes and 3 different aspect\n",
    "# # ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# # map could potentially have different sizes and\n",
    "# # aspect ratios\n",
    "# anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "#                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# # let's define what are the feature maps that we will\n",
    "# # use to perform the region of interest cropping, as well as\n",
    "# # the size of the crop after rescaling.\n",
    "# # if your backbone returns a Tensor, featmap_names is expected to\n",
    "# # be [0]. More generally, the backbone should return an\n",
    "# # OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# # feature maps to use.\n",
    "# roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "#                                                 output_size=7,\n",
    "#                                                 sampling_ratio=2)\n",
    "\n",
    "# # put the pieces together inside a FasterRCNN model\n",
    "# model = FasterRCNN(backbone,\n",
    "#                    num_classes=2,\n",
    "#                    rpn_anchor_generator=anchor_generator,\n",
    "#                    box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "labeled_scene_index = np.arange(106, 134)\n",
    "train_labeled_scene_index, val_labeled_scene_index  = gen_train_val_index(labeled_scene_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    #'first_dim': 'sample',\n",
    "    'transform': transforms.ToTensor(),\n",
    "    'image_folder': image_folder,\n",
    "    'annotation_file': annotation_csv,\n",
    "    'extra_info': True}\n",
    "\n",
    "#dataset_train = LabeledDataset_RCNN (scene_index=train_labeled_scene_index, **kwargs)\n",
    "#dataset_val = LabeledDataset_RCNN (scene_index=val_labeled_scene_index, **kwargs)\n",
    " \n",
    "dataset_train = LabeledDataset(scene_index=train_labeled_scene_index, **kwargs)\n",
    "dataset_val = LabeledDataset(scene_index=val_labeled_scene_index, **kwargs)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mod = torchvision.models.resnet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (sample, old_targets, road_image, extra) in enumerate(val_data_loader): #, print_freq, header): \n",
    "        \n",
    "        images = sample\n",
    "        targets = trans_target(old_targets)\n",
    "        #print(\"images len {}, targets len {}\".format(len(images), len(targets)))\n",
    "        #print(\"images[0] shape {}\".format(images[0].shape)) # [6, 3, 256, 306]      \n",
    "        #images = list(image.to(device) for image in images)\n",
    "        #targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample2 = [s.sum(dim=0).squeeze(0).to(device) for s in sample]\n",
    "# targets2 = [{k: v.to(device) for k, v in t.items()} for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.rpn import RPNHead\n",
    "def fr50_Model(pretrained = False):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=pretrained) #true works\n",
    "    # create an anchor_generator for the FPN\n",
    "    # which by default has 5 outputs\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        #sizes=tuple([(16, 32, 64, 128, 256, 512) for _ in range(5)]),\n",
    "        sizes=tuple([(10, 15, 20, 30, 40) for _ in range(5)]),\n",
    "         \n",
    "        aspect_ratios=tuple([(0.25, 0.5, 1.0, 2.0) for _ in range(5)]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.rpn.anchor_generator = anchor_generator\n",
    "\n",
    "    # 256 because that's the number of features that FPN returns\n",
    "    model.rpn.head = RPNHead(256, anchor_generator.num_anchors_per_location()[0])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to(device)\n",
    "\n",
    "# fastRCNNout = model(sample2, targets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastRCNNout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample3 = [s.sum(dim=0).squeeze(0).to(device) for s in sample]\n",
    "# targets3 = [{k: v.to(device) for k, v in t.items()} for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#      prediction = model(sample3, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction #no pretrained it doens't predict anything, but that' sfine\n",
    "# [{'boxes': tensor([], device='cuda:0', size=(0, 4)),\n",
    "#   'labels': tensor([], device='cuda:0', dtype=torch.int64),\n",
    "#   'scores': tensor([], device='cuda:0')}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3, 256, 306])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tt = transforms.Compose([transforms.Resize((800, 800)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample3_sew = [tt(sew_images(s)).to(device) for s in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#      prediction_sew = model(sample3_sew, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_sew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try training the a model \n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2 #4 returned classifier as nan\n",
    "\n",
    "# get the model using our helper function\n",
    "#model = get_instance_segmentation_model(num_classes)\n",
    "model = fr50_Model(True) #try true first\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0001, #reduce from 0.005 to help with the classifer loss = nan issue\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/2394]  eta: 0:52:42  lr: 0.000100  loss: 69.2373 (69.2373)  loss_classifier: 0.0570 (0.0570)  loss_box_reg: 0.0006 (0.0006)  loss_objectness: 0.6901 (0.6901)  loss_rpn_box_reg: 68.4895 (68.4895)  time: 1.3211  data: 0.5603  max mem: 5066\n",
      "Epoch: [0]  [ 100/2394]  eta: 0:14:52  lr: 0.000100  loss: 73.1177 (76.3359)  loss_classifier: 0.1342 (0.1124)  loss_box_reg: 0.0015 (0.0012)  loss_objectness: 0.6599 (0.6769)  loss_rpn_box_reg: 72.3232 (75.5454)  time: 0.3794  data: 0.0071  max mem: 5066\n",
      "Epoch: [0]  [ 200/2394]  eta: 0:13:50  lr: 0.000100  loss: 44.7692 (72.0881)  loss_classifier: 0.1122 (0.0954)  loss_box_reg: 0.0012 (0.0010)  loss_objectness: 0.0029 (0.5230)  loss_rpn_box_reg: 44.6627 (71.4687)  time: 0.3553  data: 0.0073  max mem: 5066\n",
      "Epoch: [0]  [ 300/2394]  eta: 0:13:00  lr: 0.000100  loss: 28.2854 (60.5151)  loss_classifier: 0.0487 (0.0932)  loss_box_reg: 0.0005 (0.0010)  loss_objectness: 0.0019 (0.3504)  loss_rpn_box_reg: 28.2348 (60.0705)  time: 0.3668  data: 0.0079  max mem: 5066\n",
      "Epoch: [0]  [ 400/2394]  eta: 0:12:17  lr: 0.000100  loss: 23.7400 (52.6846)  loss_classifier: 0.0346 (0.0852)  loss_box_reg: 0.0004 (0.0010)  loss_objectness: 0.0016 (0.2640)  loss_rpn_box_reg: 23.7023 (52.3345)  time: 0.3521  data: 0.0085  max mem: 5066\n",
      "Epoch: [0]  [ 500/2394]  eta: 0:11:36  lr: 0.000100  loss: 25.4116 (47.2014)  loss_classifier: 0.4007 (0.1222)  loss_box_reg: 0.0041 (0.0013)  loss_objectness: 0.0024 (0.2117)  loss_rpn_box_reg: 25.3256 (46.8662)  time: 0.3700  data: 0.0074  max mem: 5066\n",
      "Epoch: [0]  [ 600/2394]  eta: 0:11:02  lr: 0.000100  loss: 22.0628 (43.5291)  loss_classifier: 0.2921 (0.1662)  loss_box_reg: 0.0028 (0.0017)  loss_objectness: 0.0027 (0.1769)  loss_rpn_box_reg: 21.7573 (43.1843)  time: 0.3731  data: 0.0081  max mem: 5066\n",
      "Epoch: [0]  [ 700/2394]  eta: 0:10:29  lr: 0.000100  loss: 22.8195 (40.8194)  loss_classifier: 0.2796 (0.2219)  loss_box_reg: 0.0024 (0.0022)  loss_objectness: 0.0032 (0.1520)  loss_rpn_box_reg: 22.0035 (40.4433)  time: 0.4031  data: 0.0076  max mem: 5066\n",
      "Epoch: [0]  [ 800/2394]  eta: 0:09:50  lr: 0.000100  loss: 26.8868 (38.9788)  loss_classifier: 0.0948 (0.2751)  loss_box_reg: 0.0007 (0.0027)  loss_objectness: 0.0122 (0.1335)  loss_rpn_box_reg: 26.7856 (38.5675)  time: 0.3648  data: 0.0081  max mem: 5066\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-2c1b5fbed149>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_one_epoch_FastRCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/dl2020/RCNN/CP_helper_RCNN.py\u001b[0m in \u001b[0;36mtrain_one_epoch_FastRCNN\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/gpu/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/gpu/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch_FastRCNN(model, optimizer, train_data_loader, device, epoch, print_freq=100)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset - this has issues\n",
    "    #evaluate(model, val_data_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give me another one\n",
    "tt = transforms.Compose([transforms.Resize((800, 800)), transforms.ToTensor()]) #this is for 6 images combo\n",
    "for i, (sample, old_targets, road_image, extra) in enumerate(val_data_loader): #, print_freq, header): \n",
    "        \n",
    "        images = [tt(sew_images(s)).to(device) for s in sample] #list of [3, 800, 800], should be 1 per patch\n",
    "        targets = trans_target(old_targets)\n",
    "        \n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "     prediction = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([], device='cuda:0', size=(0, 4)),\n",
       "  'labels': tensor([], device='cuda:0', dtype=torch.int64),\n",
       "  'scores': tensor([], device='cuda:0')}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[ 587.7640, -410.7347,  634.5888, -390.4039],\n",
       "          [ 723.5701, -515.9766,  770.2949, -494.8876],\n",
       "          [ 346.5826, -643.8907,  392.2856, -625.2436],\n",
       "          [ 208.3556, -448.8516,  252.5559, -429.6745],\n",
       "          [ 103.3553, -449.4175,  147.5497, -430.6902],\n",
       "          [  31.1752, -483.2730,   79.2614, -463.0602],\n",
       "          [ 340.5685, -484.8769,  384.6458, -465.9312],\n",
       "          [ 648.6622, -573.7671,  696.0050, -555.0985],\n",
       "          [ 496.1005, -604.4378,  541.8034, -585.7907],\n",
       "          [ 519.1313, -483.1805,  561.6338, -464.6352],\n",
       "          [  25.9092, -382.5251,   71.8441, -359.7119],\n",
       "          [ 716.3273, -636.7295,  763.7090, -617.2219],\n",
       "          [ 321.8503, -448.2401,  395.7314, -425.9930],\n",
       "          [ 271.5695, -608.9213,  317.2724, -590.2742],\n",
       "          [ 384.5264, -608.4007,  430.2294, -589.7536],\n",
       "          [  66.1751, -413.8457,  110.3829, -394.0988],\n",
       "          [ 256.3764, -411.0828,  300.9099, -392.4210],\n",
       "          [ 692.8306, -481.0863,  740.1176, -459.0505],\n",
       "          [ 504.9691, -568.7386,  550.6720, -550.0915],\n",
       "          [  16.2913, -451.1110,   60.4900, -432.0538]], device='cuda:0'),\n",
       "  'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         device='cuda:0'),\n",
       "  'masks': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       "  'image_id': tensor([100], device='cuda:0'),\n",
       "  'area': tensor([ 951.9855,  985.3815,  852.2305,  847.6347,  827.6429,  971.9577,\n",
       "           835.0731,  883.8234,  852.2271,  788.2211, 1047.9236,  924.3049,\n",
       "          1643.6449,  852.2271,  852.2277,  872.9662,  831.0745, 1042.0093,\n",
       "           852.2271,  842.3036], device='cuda:0'),\n",
       "  'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         device='cuda:0')}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
