{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#for image transform\n",
    "#import cv2\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "sys.path.insert(1, '//workspace/dl2020')\n",
    "#from model_loader_CP2 import *\n",
    "from CP_helper_RCNN import *\n",
    "from data_helper_RCNN import *\n",
    "\n",
    "#from Unet import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    " \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "#from engine import train_one_epoch, evaluate\n",
    "\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.58 ms\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);\n",
    "image_folder = '../data'\n",
    "annotation_csv = '../data/annotation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.12 ms\n"
     ]
    }
   ],
   "source": [
    "# import torchvision\n",
    "# from torchvision.models.detection import FasterRCNN\n",
    "# from torchvision.models.detection.rpn import AnchorGenerator\n",
    "# from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "\n",
    "# # load a pre-trained model for classification and return\n",
    "# # only the features\n",
    "# #backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# backbone = resnet_fpn_backbone('resnet18', pretrained = False)\n",
    "# # FasterRCNN needs to know the number of\n",
    "# # output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# # so we need to add it here\n",
    "# #backbone.out_channels = 1280\n",
    "# backbone.out_channels = 256 #this is a guess\n",
    "\n",
    "\n",
    "# # let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# # location, with 5 different sizes and 3 different aspect\n",
    "# # ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# # map could potentially have different sizes and\n",
    "# # aspect ratios\n",
    "# anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "#                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# # let's define what are the feature maps that we will\n",
    "# # use to perform the region of interest cropping, as well as\n",
    "# # the size of the crop after rescaling.\n",
    "# # if your backbone returns a Tensor, featmap_names is expected to\n",
    "# # be [0]. More generally, the backbone should return an\n",
    "# # OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# # feature maps to use.\n",
    "# roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "#                                                 output_size=7,\n",
    "#                                                 sampling_ratio=2)\n",
    "\n",
    "# # put the pieces together inside a FasterRCNN model\n",
    "# model = FasterRCNN(backbone,\n",
    "#                    num_classes=2,\n",
    "#                    rpn_anchor_generator=anchor_generator,\n",
    "#                    box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.75 ms\n"
     ]
    }
   ],
   "source": [
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "labeled_scene_index = np.arange(106, 134)\n",
    "train_labeled_scene_index, val_labeled_scene_index  = gen_train_val_index(labeled_scene_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 824 ms\n"
     ]
    }
   ],
   "source": [
    "#normalize = torchvision.transforms.Normalize(mean=[0.6394939, 0.6755114, 0.7049375],\n",
    "#                                     std=[0.31936955, 0.3117349 , 0.2953726 ])\n",
    "\n",
    "#transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "#                                           normalize\n",
    "#                                           ])\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "kwargs = {\n",
    "    #'first_dim': 'sample',\n",
    "    'transform': transform,\n",
    "    'image_folder': image_folder,\n",
    "    'annotation_file': annotation_csv,\n",
    "    'extra_info': True}\n",
    "\n",
    "#dataset_train = LabeledDataset_RCNN (scene_index=train_labeled_scene_index, **kwargs)\n",
    "#dataset_val = LabeledDataset_RCNN (scene_index=val_labeled_scene_index, **kwargs)\n",
    "\n",
    "dataset_train = LabeledDataset(scene_index=train_labeled_scene_index, **kwargs)\n",
    "dataset_val = LabeledDataset(scene_index=val_labeled_scene_index, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.77 ms\n"
     ]
    }
   ],
   "source": [
    "#mod = torchvision.models.resnet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.04 s\n"
     ]
    }
   ],
   "source": [
    "#check one sample from validation\n",
    "for i, (sample, old_targets, road_image, extra) in enumerate(val_data_loader): #, print_freq, header): \n",
    "        \n",
    "        images = sample\n",
    "        targets = trans_target(old_targets)\n",
    "        #print(\"images len {}, targets len {}\".format(len(images), len(targets)))\n",
    "        #print(\"images[0] shape {}\".format(images[0].shape)) # [6, 3, 256, 306]      \n",
    "        #images = list(image.to(device) for image in images)\n",
    "        #targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.7 ms\n"
     ]
    }
   ],
   "source": [
    "normalize = torchvision.transforms.Normalize(mean=[0.6394939, 0.6755114, 0.7049375],\n",
    "                                 std=[0.31936955, 0.3117349 , 0.2953726 ])\n",
    "\n",
    "tt = transforms.Compose([transforms.Resize((800, 800)), transforms.ToTensor(), normalize]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 187 ms\n"
     ]
    }
   ],
   "source": [
    "images = [tt(sew_images(s)) for s in sample] #list of [3, 800, 800], should be 1 per patch\n",
    "targets = trans_target(old_targets)\n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 800, 800])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.6 ms\n"
     ]
    }
   ],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1288)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 14.8 ms\n"
     ]
    }
   ],
   "source": [
    "images[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[587.7640, 390.4039, 634.5888, 410.7347],\n",
       "          [723.5701, 494.8876, 770.2949, 515.9766],\n",
       "          [346.5826, 625.2436, 392.2856, 643.8907],\n",
       "          [208.3556, 429.6745, 252.5559, 448.8516],\n",
       "          [103.3553, 430.6902, 147.5497, 449.4175],\n",
       "          [ 31.1752, 463.0602,  79.2614, 483.2730],\n",
       "          [340.5685, 465.9312, 384.6458, 484.8769],\n",
       "          [648.6622, 555.0985, 696.0050, 573.7671],\n",
       "          [496.1005, 585.7907, 541.8034, 604.4378],\n",
       "          [519.1313, 464.6352, 561.6338, 483.1805],\n",
       "          [ 25.9092, 359.7119,  71.8441, 382.5251],\n",
       "          [716.3273, 617.2219, 763.7090, 636.7295],\n",
       "          [321.8503, 425.9930, 395.7314, 448.2401],\n",
       "          [271.5695, 590.2742, 317.2724, 608.9213],\n",
       "          [384.5264, 589.7536, 430.2294, 608.4007],\n",
       "          [ 66.1751, 394.0988, 110.3829, 413.8457],\n",
       "          [256.3764, 392.4210, 300.9099, 411.0828],\n",
       "          [692.8306, 459.0505, 740.1176, 481.0863],\n",
       "          [504.9691, 550.0915, 550.6720, 568.7386],\n",
       "          [ 16.2913, 432.0538,  60.4900, 451.1110]]),\n",
       "  'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'masks': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
       "  'image_id': tensor([100]),\n",
       "  'area': tensor([ 951.9855,  985.3815,  852.2305,  847.6347,  827.6429,  971.9577,\n",
       "           835.0731,  883.8234,  852.2271,  788.2211, 1047.9236,  924.3049,\n",
       "          1643.6449,  852.2271,  852.2277,  872.9662,  831.0745, 1042.0093,\n",
       "           852.2271,  842.3036]),\n",
       "  'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23.6 ms\n"
     ]
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390 411 588 635\n",
      "495 516 724 770\n",
      "625 644 347 392\n",
      "430 449 208 253\n",
      "431 449 103 148\n",
      "463 483 31 79\n",
      "466 485 341 385\n",
      "555 574 649 696\n",
      "586 604 496 542\n",
      "465 483 519 562\n",
      "360 383 26 72\n",
      "617 637 716 764\n",
      "426 448 322 396\n",
      "590 609 272 317\n",
      "590 608 385 430\n",
      "394 414 66 110\n",
      "392 411 256 301\n",
      "459 481 693 740\n",
      "550 569 505 551\n",
      "432 451 16 60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f20267d9e10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "#try plotting the targets\n",
    "\n",
    "road_img = gen_result_chart(targets[0])\n",
    "fig, ax = plt.subplots()\n",
    "#color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    " \n",
    "ax.imshow(road_img, cmap ='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfsa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e6f2d5759121>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfsa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dfsa' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 761 ms\n"
     ]
    }
   ],
   "source": [
    "dfsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.rpn import RPNHead\n",
    "def fr50_Model(pretrained = False):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=pretrained) #true works\n",
    "    # create an anchor_generator for the FPN\n",
    "    # which by default has 5 outputs\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        #sizes=tuple([(16, 32, 64, 128, 256, 512) for _ in range(5)]),\n",
    "        sizes=tuple([(10, 15, 20, 30, 40) for _ in range(5)]),\n",
    "         \n",
    "        aspect_ratios=tuple([(0.25, 0.5, 1.0, 2.0) for _ in range(5)]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.rpn.anchor_generator = anchor_generator\n",
    "\n",
    "    # 256 because that's the number of features that FPN returns\n",
    "    model.rpn.head = RPNHead(256, anchor_generator.num_anchors_per_location()[0])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try training the a model with sew images non pannorm\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2 #4 returned classifier as nan\n",
    "\n",
    "# get the model using our helper function\n",
    "#model = get_instance_segmentation_model(num_classes)\n",
    "model = fr50_Model(True) #try true first\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0001, #reduce from 0.005 to help with the classifer loss = nan issue\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the output before training\n",
    "images = [m.to(device) for m in images]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "     prediction = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch_FastRCNN(model, optimizer, train_data_loader, device, epoch, print_freq=100)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset - this has issues\n",
    "    #evaluate(model, val_data_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give me another one\n",
    "normalize = torchvision.transforms.Normalize(mean=[0.6394939, 0.6755114, 0.7049375],\n",
    "                                 std=[0.31936955, 0.3117349 , 0.2953726 ])\n",
    "\n",
    "tt = transforms.Compose([transforms.Resize((800, 800)), transforms.ToTensor(), normalize]) #this is for 6 images combo\n",
    "for i, (sample, old_targets, road_image, extra) in enumerate(val_data_loader): #, print_freq, header): \n",
    "        \n",
    "        images = [tt(sew_images(s)).to(device) for s in sample] #list of [3, 800, 800], should be 1 per patch\n",
    "        targets = trans_target(old_targets)\n",
    "        \n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "     prediction = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "208.3556, 429.6745, 252.5559, 448.8516]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "          1, 1, 1, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#works with sewing 6 images with \n",
    "torch.save({\n",
    "\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            }, \"../models/fastRCNN_sew61epoch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try with panoramic images but reshaped to 800\n",
    "\n",
    "for i, (sample, old_targets, road_image, extra) in enumerate(val_data_loader): #, print_freq, header): \n",
    "        \n",
    "        images = sew_images_panorm(sample) #list of [3, 800, 800], should be 1 per patch\n",
    "        targets = trans_target(old_targets)\n",
    "        \n",
    "        #targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up new training\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2 #4 returned classifier as nan\n",
    "\n",
    "# get the model using our helper function\n",
    "#model = get_instance_segmentation_model(num_classes)\n",
    "model = fr50_Model(True) #try true first\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0001, #reduce from 0.005 to help with the classifer loss = nan issue\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch_FastRCNN(model, optimizer, train_data_loader, device, epoch, print_freq=100, panorm = True)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset - this has issues\n",
    "    #evaluate(model, val_data_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate on 1 sample\n",
    "\n",
    "for i, (sample, old_targets, road_image, extra) in enumerate(val_data_loader): #, print_freq, header): \n",
    "        \n",
    "        images = [tt(s).to(device) for s in sew_images_panorm(sample)] #list of [3, 800, 800], should be 1 per patch\n",
    "        targets = trans_target(old_targets)\n",
    "        \n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "     prediction = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "          1, 1, 1, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#works with sewing 6 images with \n",
    "torch.save({\n",
    "\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            }, \"../models/fastRCNN_sewpanorm1epoch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try one without pretrained with reg sew images non panorm\n",
    " \n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2 #4 returned classifier as nan\n",
    "\n",
    "# get the model using our helper function\n",
    "#model = get_instance_segmentation_model(num_classes)\n",
    "model = fr50_Model(False) #try true first\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0001, #reduce from 0.005 to help with the classifer loss = nan issue\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch_FastRCNN(model, optimizer, train_data_loader, device, epoch, print_freq=100)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate on 1 sample\n",
    "\n",
    "for i, (sample, old_targets, road_image, extra) in enumerate(val_data_loader): #, print_freq, header): \n",
    "        \n",
    "        images = [tt(sew_images(s)).to(device) for s in sample] #list of [3, 800, 800], should be 1 per patch\n",
    "        targets = trans_target(old_targets)\n",
    "        \n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "     prediction = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
